{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KollektifProje_Bagging_RF_AB_CV",
      "provenance": [],
      "collapsed_sections": [
        "_vgNW6ZBvEJM"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7ftrIKBvEQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b54e6c-60d2-481e-abb9-cf1b551ded07"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pylab import savefig\r\n",
        "from scipy.io import arff\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "import ntpath\r\n",
        "import glob\r\n",
        "import os\r\n",
        "import math\r\n",
        "# !pip install liac-arff\r\n",
        "#import arff\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from random import randrange\r\n",
        "from itertools import combinations as comb\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.ensemble import VotingClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "import string\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouZ_LPRzlfD1"
      },
      "source": [
        "def preprocess(listoftext):\r\n",
        "    customstopwords = nltk.corpus.stopwords.words('english')\r\n",
        "    customstopwords.append('rt')\r\n",
        "    emojipattern = re.compile(\"[\"\r\n",
        "                              u\"\\U0001F600-\\U0001F64F\"\r\n",
        "                              u\"\\U0001F300-\\U0001F5FF\"\r\n",
        "                              u\"\\U0001F680-\\U0001F6FF\"\r\n",
        "                              u\"\\U0001F1E0-\\U0001F1FF\"\r\n",
        "                              u\"\\U00000430-\\U00000648\"\r\n",
        "                              u\"\\U0000263a-\\U0000fe0f\"\r\n",
        "                              u\"\\U0000201c-\\U0000201d\"\r\n",
        "                              u\"\\U00002000-\\U000020e3\"\r\n",
        "                              u\"\\U0000064a-\\U0000064d\"\r\n",
        "                              u\"\\U0000ff00-\\U0000ff09\"\r\n",
        "                              u\"\\U000fe520-\\U000fe529\"\r\n",
        "                              u\"\\U0000221b-\\U0000221e\"\r\n",
        "                              u\"\\U000feb90-\\U000feb99\"\r\n",
        "                              u\"\\U00001d20-\\U00001d4c\"\r\n",
        "                              u\"\\U00000080-\\U000000FF\"\r\n",
        "                              u\"\\U00000100-\\U00000139\"\r\n",
        "                              u\"\\U0000FF80-\\U0001007F\"\"]+\", flags=re.UNICODE)\r\n",
        "    httppattern = re.compile('http?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\r\n",
        "\r\n",
        "    punctuationremoved = [char for char in listoftext if char not in  string.punctuation]\r\n",
        "    punctuationremoved = ''.join(punctuationremoved)\r\n",
        "\r\n",
        "    advancedremoved = emojipattern.sub(r'', punctuationremoved)\r\n",
        "    advancedremoved = httppattern.sub(r'', advancedremoved)\r\n",
        "\r\n",
        "    stopwordsremoved = [word for word in advancedremoved.split() if word.lower() not in customstopwords]\r\n",
        "    return stopwordsremoved"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BybuTagCAUwV"
      },
      "source": [
        "# <h1> Datasets</h1>\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suGHsjVS10uc"
      },
      "source": [
        "ds_folderpath = \"gdrive/MyDrive/ImprovedSpace/arfffiles/\"\n",
        "ds_files = glob.glob(ds_folderpath + \"/*.arff\")\n",
        "datasets = {} # dictionary\n",
        "for file_path in ds_files:\n",
        "  filename = os.path.splitext(ntpath.basename(file_path))[0]\n",
        "  if filename != \"cnae\" and filename != \"cnae_utf\": # cnae veriseti utf olmadığı için, eskisi yerine aynı içerikte, utf encoded hali kaydedidi: cnae_utf.arff \n",
        "    filedata, meta = arff.loadarff(file_path)\n",
        "    attrNames = meta.names()\n",
        "    classname = attrNames[-1]\n",
        "    attrnames = attrNames[0:-1]\n",
        "    \n",
        "    data = pd.DataFrame(filedata)\n",
        "    samples = data[attrnames]\n",
        "    labels = data[classname]\n",
        "    datasets[filename] = (samples, labels)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8DFFT6nlpaK"
      },
      "source": [
        "ds_folderpath = \"gdrive/MyDrive/ImprovedSpace/\"\r\n",
        "ds_files = glob.glob(ds_folderpath + \"/\")\r\n",
        "\r\n",
        "datasets[\"iphonetweets\"] = pd.read_csv(ds_folderpath+'iphonetweets.csv', sep=',', names=['text', 'label'], usecols=['text', 'label'])\r\n",
        "datasets[\"hobbittweets\"] = pd.read_csv(ds_folderpath+'Hobbittweets.csv', sep=',', names=['text', 'label'],usecols=['text', 'label'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Z4J48tA97u"
      },
      "source": [
        "Functions \r\n",
        "----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-F5iBZ08nuK"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def x2fx(x, model='quadratic'):\r\n",
        "    linear = np.c_[np.ones(x.shape[0]), x]\r\n",
        "    if model == 'linear':\r\n",
        "        return linear\r\n",
        "    if model == 'purequadratic':\r\n",
        "        return np.c_[linear, x**2]\r\n",
        "    interaction = np.array([x[:,i]*x[:,j] for i, j in comb(range(x.shape[1]), 2)]).T\r\n",
        "    if model == 'interaction':\r\n",
        "        return np.c_[linear, interaction]\r\n",
        "    if model == 'quadratic':\r\n",
        "        return np.c_[linear, interaction, x**2]\r\n",
        "\r\n",
        "def generate_imp_space(X_train, Y_train, X_test, imp_feature_size, foz):\r\n",
        "  imp_train_data = X_train.values\r\n",
        "  imp_test_data = X_test.values\r\n",
        "  d = len(X_train.columns)\r\n",
        "  # print(\"____1_____\")\r\n",
        "  for i in range(0,imp_feature_size*foz):\r\n",
        "    Xindis = np.random.permutation(d)\r\n",
        "    for j in range(0,d-(foz-1),foz):  #d/foz kadar doner\r\n",
        "      sX = np.random.permutation(num_class)\r\n",
        "      s1 = sX[0]\r\n",
        "      # print(\"____2_____\")\r\n",
        "      s1data = X_train[X_train.index.isin(Y_train[Y_train == str(s1)].index)]\r\n",
        "      s2data = X_train[~X_train.index.isin(Y_train[Y_train == str(s1)].index)]\r\n",
        "      s1data = s1data.iloc[:,Xindis[j:j+(foz)]]\r\n",
        "      s2data = s2data.iloc[:,Xindis[j:j+(foz)]] # s1 vs all other classes, #foz feature\r\n",
        "      # print(\"____3_____\")\r\n",
        "      s1label = np.ones((s1data.values.shape[0],1),dtype=int)\r\n",
        "      s2label = -1*np.ones((s2data.values.shape[0],1),dtype=int)\r\n",
        "      Wdata = np.concatenate((s1data,s2data))\r\n",
        "      # print(\"____4_____\")\r\n",
        "      \r\n",
        "      Wdata = x2fx(Wdata)\r\n",
        "      Wlabel = np.concatenate((s1label,s2label))\r\n",
        "      W = np.matmul(np.matmul(np.linalg.pinv(np.matmul(Wdata.T, Wdata)),Wdata.T),Wlabel)\r\n",
        "      \r\n",
        "      WW = x2fx(X_train.iloc[:,Xindis[j:j+(foz)]].values)\r\n",
        "      imp_train_data = np.concatenate((imp_train_data, np.matmul(WW,W)),axis=1)\r\n",
        "      \r\n",
        "      TT = x2fx(X_test.iloc[:,Xindis[j:j+(foz)]].values)\r\n",
        "      imp_test_data = np.concatenate((imp_test_data, np.matmul(TT,W)),axis=1)\r\n",
        "    \r\n",
        "  return imp_train_data,imp_test_data\r\n",
        "\r\n",
        "# Create a random subsample from the dataset with replacement\r\n",
        "\r\n",
        "def subsample(X, X_imp, Y, ratio):\r\n",
        "    xsample = list()\r\n",
        "    ximpsample = list()\r\n",
        "    labels = list()\r\n",
        "    n_sample = round(len(X) * ratio)\r\n",
        "    while len(xsample) < n_sample:\r\n",
        "        index = randrange(len(X))\r\n",
        "        xsample.append(X[index])\r\n",
        "        ximpsample.append(X_imp[index])\r\n",
        "        labels.append(Y[index])\r\n",
        "    return np.array(xsample),np.array(ximpsample),np.array(labels)\r\n",
        "\r\n",
        "\r\n",
        "def MajorityVoting(votes):\r\n",
        "  results = []\r\n",
        "  for i in range(0,votes.shape[1]):\r\n",
        "    values, counts = np.unique(votes[:,i], return_counts=True)\r\n",
        "    results.append(values[np.argmax(counts)])\r\n",
        "  return np.array(results)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-kHn-AKCUa"
      },
      "source": [
        "Random Forest\r\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkOllNmFZ3vw"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\r\n",
        "from sklearn.model_selection import RepeatedKFold\r\n",
        "\r\n",
        "\r\n",
        "foz=4\r\n",
        "imp_feature_size=1\r\n",
        "n_estimators = 10\r\n",
        "#Table\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "pt_RF = PrettyTable()\r\n",
        "\r\n",
        "pt_RF.field_names = [\"Dataset\", \"Imp_RF\", \"RF\"]\r\n",
        "#\r\n",
        "foldPreds_RF = {}\r\n",
        "K =0\r\n",
        "for ds in datasets:\r\n",
        "  K+=1\r\n",
        "  rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=2652124)\r\n",
        "  if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "    df = datasets[ds]\r\n",
        "    X = df['text']\r\n",
        "    Y = df['label']\r\n",
        "  else:\r\n",
        "    X,Y = datasets[ds]\r\n",
        "    Y = Y.str.decode(\"utf-8\")\r\n",
        "    \r\n",
        "  # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\r\n",
        "  print(ds)\r\n",
        "  accuracies = []\r\n",
        "  accuracies_imp = []  \r\n",
        "  for train_index, test_index in rkf.split(X):\r\n",
        "\r\n",
        "    # X_train, X_test = X[train_index], X[test_index]\r\n",
        "    # Y_train, Y_test = Y[train_index], Y[test_index]\r\n",
        "    X_train = X.iloc[train_index]\r\n",
        "    X_test = X.iloc[test_index]\r\n",
        "    Y_train = Y.iloc[train_index]\r\n",
        "    Y_test = Y.iloc[test_index]\r\n",
        "\r\n",
        "    if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "      vectorizer = CountVectorizer(analyzer=preprocess)\r\n",
        "      vectorizer.fit(X_train)\r\n",
        "\r\n",
        "      X_train = vectorizer.transform(X_train).toarray()\r\n",
        "      X_test = vectorizer.transform(X_test).toarray()\r\n",
        "      X_train = pd.DataFrame(X_train)\r\n",
        "      X_test = pd.DataFrame(X_test)\r\n",
        "\r\n",
        "    d = len(X_train.columns)\r\n",
        "    num_class = len(Y_train.value_counts())\r\n",
        "\r\n",
        "\r\n",
        "    imp_tree_predicts = []\r\n",
        "    tree_predicts = []\r\n",
        "\r\n",
        "    for i in range(0,n_estimators):\r\n",
        "      imp_tr, imp_ts = generate_imp_space(X_train, Y_train, X_test, imp_feature_size, foz)\r\n",
        "      imp_d = imp_tr.shape[1]\r\n",
        "\r\n",
        "      #meta learner params\r\n",
        "      imp_sel_d = 2* round(math.log2(imp_d)) #feature\r\n",
        "      sel_d = 2*round(math.log2(d))\r\n",
        "      \r\n",
        "      imp_tree = RandomForestClassifier(max_features=imp_sel_d, n_estimators=1)#,random_state=42\r\n",
        "      imp_tree.fit(imp_tr, Y_train)\r\n",
        "      imp_tree_predicts.append(imp_tree.predict(imp_ts))\r\n",
        "\r\n",
        "\r\n",
        "      tree = RandomForestClassifier(max_features=sel_d, n_estimators=1)#, random_state=42\r\n",
        "      tree.fit(X_train, Y_train)\r\n",
        "      tree_predicts.append(tree.predict(X_test))\r\n",
        "\r\n",
        "    results_imp = MajorityVoting(np.array(imp_tree_predicts))\r\n",
        "    results = MajorityVoting(np.array(tree_predicts))\r\n",
        "\r\n",
        "    # print(\"--------------  {}  ----------------\".format(ds.upper()))\r\n",
        "    accuracies.append(accuracy_score(Y_test.values, results))\r\n",
        "    accuracies_imp.append(accuracy_score(Y_test.values, results_imp))\r\n",
        "    \r\n",
        "    pt_RF.add_row((ds, \"%.4f\" % accuracy_score(Y_test.values, results_imp),  \"%.4f\" % accuracy_score(Y_test.values, results)))\r\n",
        "  foldPreds_RF[ds] = accuracies\r\n",
        "  foldPreds_RF[ds+\"_imp\"] = accuracies_imp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d4YlaogzCKb"
      },
      "source": [
        "print(pt_RF) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5md9xSXlqbcV"
      },
      "source": [
        "np.save(ds_folderpath + \"/\" + 'RF.npy',foldPreds_RF ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxskWdO9mg5U"
      },
      "source": [
        "Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK4ZUtX7vCE5"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\r\n",
        "from sklearn.model_selection import RepeatedKFold\r\n",
        "foz=4\r\n",
        "imp_feature_size=1\r\n",
        "n_estimators = 10\r\n",
        "#Table\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "pt_Bagging = PrettyTable()\r\n",
        "\r\n",
        "pt_Bagging.field_names = [\"Dataset\", \"Imp_Bagging\", \"Bagging\"]\r\n",
        "#\r\n",
        "foldPreds_Bagging = {}\r\n",
        "K =0\r\n",
        "for ds in datasets:\r\n",
        "  K+=1\r\n",
        "  rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=2652124)\r\n",
        "  if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "    df = datasets[ds]\r\n",
        "    X = df['text']\r\n",
        "    Y = df['label']\r\n",
        "  else:\r\n",
        "    X,Y = datasets[ds]\r\n",
        "    Y = Y.str.decode(\"utf-8\")\r\n",
        "    \r\n",
        "  # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\r\n",
        "  print(ds)\r\n",
        "  accuracies = []\r\n",
        "  accuracies_imp = []  \r\n",
        "  for train_index, test_index in rkf.split(X):\r\n",
        "\r\n",
        "    X_train = X.iloc[train_index]\r\n",
        "    X_test = X.iloc[test_index]\r\n",
        "    Y_train = Y.iloc[train_index]\r\n",
        "    Y_test = Y.iloc[test_index]\r\n",
        "\r\n",
        "    if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "      vectorizer = CountVectorizer(analyzer=preprocess)\r\n",
        "      vectorizer.fit(X_train)\r\n",
        "\r\n",
        "      X_train = vectorizer.transform(X_train).toarray()\r\n",
        "      X_test = vectorizer.transform(X_test).toarray()\r\n",
        "      X_train = pd.DataFrame(X_train)\r\n",
        "      X_test = pd.DataFrame(X_test)\r\n",
        "\r\n",
        "    d = len(X_train.columns)\r\n",
        "    num_class = len(Y_train.value_counts())\r\n",
        "\r\n",
        "\r\n",
        "    imp_tree_predicts = []\r\n",
        "    tree_predicts = []\r\n",
        "\r\n",
        "    for i in range(0,n_estimators):\r\n",
        "      imp_tr, imp_ts = generate_imp_space(X_train, Y_train, X_test, imp_feature_size, foz)\r\n",
        "      imp_d = imp_tr.shape[1]   \r\n",
        "      \r\n",
        "      imp_tree = BaggingClassifier(n_estimators=1)#,random_state=42\r\n",
        "      imp_tree.fit(imp_tr, Y_train)\r\n",
        "      imp_tree_predicts.append(imp_tree.predict(imp_ts))\r\n",
        "\r\n",
        "\r\n",
        "      tree = BaggingClassifier(n_estimators=1)#, random_state=42\r\n",
        "      tree.fit(X_train, Y_train)\r\n",
        "      tree_predicts.append(tree.predict(X_test))\r\n",
        "\r\n",
        "    results_imp = MajorityVoting(np.array(imp_tree_predicts))\r\n",
        "    results = MajorityVoting(np.array(tree_predicts))\r\n",
        "\r\n",
        "    # print(\"--------------  {}  ----------------\".format(ds.upper()))\r\n",
        "    accuracies.append(accuracy_score(Y_test.values, results))\r\n",
        "    accuracies_imp.append(accuracy_score(Y_test.values, results_imp))\r\n",
        "    # print(\" \")\r\n",
        "    pt_Bagging.add_row((ds, \"%.4f\" % accuracy_score(Y_test.values, results_imp),  \"%.4f\" % accuracy_score(Y_test.values, results)))\r\n",
        "\r\n",
        "  foldPreds_Bagging[ds] = accuracies\r\n",
        "  foldPreds_Bagging[ds+\"_imp\"] = accuracies_imp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeGnS84dagg7"
      },
      "source": [
        "print(pt_Bagging) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpsfLmKyUTCl"
      },
      "source": [
        "np.save(ds_folderpath + \"/\" + 'bagging.npy',foldPreds_Bagging ) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAgsYLf4qmK_"
      },
      "source": [
        "Sklearn Adaboost Fonksiyonları\r\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW3mwZeYee5S"
      },
      "source": [
        "from abc import ABCMeta, abstractmethod\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from scipy.special import xlogy\r\n",
        "\r\n",
        "from sklearn.ensemble import BaseEnsemble\r\n",
        "from sklearn.base import ClassifierMixin, RegressorMixin, is_classifier, is_regressor\r\n",
        "\r\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\r\n",
        "from sklearn.utils import check_array, check_random_state, _safe_indexing\r\n",
        "from sklearn.utils.extmath import softmax, stable_cumsum\r\n",
        "from sklearn.metrics import accuracy_score, r2_score\r\n",
        "from sklearn.utils.validation import check_is_fitted, _check_sample_weight,has_fit_parameter,_num_samples, _deprecate_positional_args\r\n",
        "\r\n",
        "# __all__ = [\r\n",
        "#     'AdaBoostClassifier',\r\n",
        "#     'AdaBoostRegressor',\r\n",
        "# ]\r\n",
        "\r\n",
        "class BaseWeightBoosting_IMP(BaseEnsemble, metaclass=ABCMeta):\r\n",
        "    @abstractmethod\r\n",
        "    def __init__(self,\r\n",
        "                 base_estimator=None, *,\r\n",
        "                 n_estimators=50,\r\n",
        "                 estimator_params=tuple(),\r\n",
        "                 learning_rate=1.,\r\n",
        "                 random_state=None):\r\n",
        "\r\n",
        "        super().__init__(\r\n",
        "            base_estimator=base_estimator,\r\n",
        "            n_estimators=n_estimators,\r\n",
        "            estimator_params=estimator_params)\r\n",
        "\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.random_state = random_state\r\n",
        "\r\n",
        "    def _check_X(self, X):\r\n",
        "        return check_array(X, accept_sparse=['csr', 'csc'], ensure_2d=True,\r\n",
        "                           allow_nd=True, dtype=None)\r\n",
        "\r\n",
        "    def fit(self, X, X_test, y, sample_weight=None):\r\n",
        "        # Check parameters\r\n",
        "        if self.learning_rate <= 0:\r\n",
        "            raise ValueError(\"learning_rate must be greater than zero\")\r\n",
        "      #  print(\"_2_\")\r\n",
        "        # X, y = self._validate_data(X, y,\r\n",
        "        #                            accept_sparse=['csr', 'csc'],\r\n",
        "        #                            ensure_2d=True,\r\n",
        "        #                            allow_nd=True,\r\n",
        "        #                            dtype=None,\r\n",
        "        #                            y_numeric=is_regressor(self))\r\n",
        "        #print(\"_3_\")\r\n",
        "\r\n",
        "        sample_weight = _check_sample_weight(sample_weight, X, np.float64)\r\n",
        "        sample_weight /= sample_weight.sum()\r\n",
        "        if np.any(sample_weight < 0):\r\n",
        "            raise ValueError(\"sample_weight cannot contain negative weights\")\r\n",
        "        #print(\"_4_\")\r\n",
        "        # Check parameters\r\n",
        "        self._validate_estimator()\r\n",
        "\r\n",
        "        # Clear any previous fit results\r\n",
        "        self.estimators_ = []\r\n",
        "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\r\n",
        "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\r\n",
        "        #print(\"_5_\")\r\n",
        "        # Initializion of the random number instance that will be used to\r\n",
        "        # generate a seed at each iteration\r\n",
        "        random_state = check_random_state(self.random_state)\r\n",
        "        #print(\"_6_\")\r\n",
        "        c = 0\r\n",
        "        for iboost in range(self.n_estimators):\r\n",
        "            c+=1\r\n",
        "           # print(\"_7_\", str(c))\r\n",
        "            # dfx = pd.DataFrame(data=X)\r\n",
        "            # dfy = pd.DataFrame(data=y)\r\n",
        "            # dfx_test = pd.DataFrame(data=X_test)\r\n",
        "           # print(\"_7_a\", str(c))\r\n",
        "\r\n",
        "            # print(dfx.values.shape)\r\n",
        "            # print(dfy.values.shape)\r\n",
        "            # print(dfx_test.values.shape)\r\n",
        "            # print(type(X))\r\n",
        "            # print(type(y))\r\n",
        "\r\n",
        "            #HER TEKIL AGAC OLUSTURULMADAN ONCE IMPROVED SPACE\r\n",
        "            imp_tr, imp_ts = generate_imp_space(X, y, X_test, imp_feature_size=1, foz=4)\r\n",
        "           # print(\"_7_b\", str(c))\r\n",
        "            # Boosting step\r\n",
        "            sample_weight, estimator_weight, estimator_error = self._boost(\r\n",
        "                iboost,\r\n",
        "                imp_tr, imp_ts, y,\r\n",
        "                sample_weight,\r\n",
        "                random_state)\r\n",
        "           # print(\"_7_c\", str(c))\r\n",
        "            # Early termination\r\n",
        "            if sample_weight is None:\r\n",
        "                break\r\n",
        "\r\n",
        "            self.estimator_weights_[iboost] = estimator_weight\r\n",
        "            self.estimator_errors_[iboost] = estimator_error\r\n",
        "\r\n",
        "            # Stop if error is zero\r\n",
        "            if estimator_error == 0:\r\n",
        "                break\r\n",
        "\r\n",
        "            sample_weight_sum = np.sum(sample_weight)\r\n",
        "\r\n",
        "            # Stop if the sum of sample weights has become non-positive\r\n",
        "            if sample_weight_sum <= 0:\r\n",
        "                break\r\n",
        "\r\n",
        "            if iboost < self.n_estimators - 1:\r\n",
        "                # Normalize\r\n",
        "                sample_weight /= sample_weight_sum\r\n",
        "\r\n",
        "        return self\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def _boost(self, iboost, X, y, sample_weight, random_state):\r\n",
        "       # print(\"_8_\")\r\n",
        "        pass\r\n",
        "\r\n",
        "    def staged_score(self, X, y, sample_weight=None):\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        for y_pred in self.staged_predict(X):\r\n",
        "            if is_classifier(self):\r\n",
        "                yield accuracy_score(y, y_pred, sample_weight=sample_weight)\r\n",
        "            else:\r\n",
        "                yield r2_score(y, y_pred, sample_weight=sample_weight)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def feature_importances_(self):\r\n",
        "        if self.estimators_ is None or len(self.estimators_) == 0:\r\n",
        "            raise ValueError(\"Estimator not fitted, \"\r\n",
        "                             \"call `fit` before `feature_importances_`.\")\r\n",
        "\r\n",
        "        try:\r\n",
        "            norm = self.estimator_weights_.sum()\r\n",
        "            return (sum(weight * clf.feature_importances_ for weight, clf\r\n",
        "                    in zip(self.estimator_weights_, self.estimators_))\r\n",
        "                    / norm)\r\n",
        "\r\n",
        "        except AttributeError as e:\r\n",
        "            raise AttributeError(\r\n",
        "                \"Unable to compute feature importances \"\r\n",
        "                \"since base_estimator does not have a \"\r\n",
        "                \"feature_importances_ attribute\") from e\r\n",
        "\r\n",
        "\r\n",
        "    def _samme_proba(self, estimator, n_classes, X):\r\n",
        "        proba = estimator.predict_proba(X)\r\n",
        "\r\n",
        "        # Displace zero probabilities so the log is defined.\r\n",
        "        # Also fix negative elements which may occur with\r\n",
        "        # negative sample weights.\r\n",
        "        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\r\n",
        "        log_proba = np.log(proba)\r\n",
        "\r\n",
        "        return (n_classes - 1) * (log_proba - (1. / n_classes)\r\n",
        "                                  * log_proba.sum(axis=1)[:, np.newaxis])\r\n",
        "\r\n",
        "\r\n",
        "################################################  ADABOOST  ########################################################################\r\n",
        "\r\n",
        "class AdaBoostClassifier_IMP(ClassifierMixin, BaseWeightBoosting_IMP):\r\n",
        "    @_deprecate_positional_args\r\n",
        "    def __init__(self,\r\n",
        "                 base_estimator=None, *,\r\n",
        "                 n_estimators=50,\r\n",
        "                 learning_rate=1.,\r\n",
        "                 algorithm='SAMME.R',\r\n",
        "                 testpredictions = [],\r\n",
        "                 random_state=None):\r\n",
        "\r\n",
        "        super().__init__(\r\n",
        "            base_estimator=base_estimator,\r\n",
        "            n_estimators=n_estimators,\r\n",
        "            learning_rate=learning_rate,\r\n",
        "            random_state=random_state)\r\n",
        "\r\n",
        "        self.algorithm = algorithm\r\n",
        "        self.testpredictions = testpredictions\r\n",
        "\r\n",
        "    def fit(self, X, X_test, y, sample_weight=None):\r\n",
        "        # Check that algorithm is supported\r\n",
        "        if self.algorithm not in ('SAMME', 'SAMME.R'):\r\n",
        "            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\r\n",
        "      #  print(\"_1_\")\r\n",
        "        # Fit\r\n",
        "        self.testpredictions = []\r\n",
        "        return super().fit(X, X_test, y, sample_weight)\r\n",
        "\r\n",
        "    def _validate_estimator(self):\r\n",
        "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\r\n",
        "        super()._validate_estimator(\r\n",
        "            default=DecisionTreeClassifier(max_depth=1))\r\n",
        "\r\n",
        "        #  SAMME-R requires predict_proba-enabled base estimators\r\n",
        "        if self.algorithm == 'SAMME.R':\r\n",
        "            if not hasattr(self.base_estimator_, 'predict_proba'):\r\n",
        "                raise TypeError(\r\n",
        "                    \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\r\n",
        "                    \"that the weak learner supports the calculation of class \"\r\n",
        "                    \"probabilities with a predict_proba method.\\n\"\r\n",
        "                    \"Please change the base estimator or set \"\r\n",
        "                    \"algorithm='SAMME' instead.\")\r\n",
        "        if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\r\n",
        "            raise ValueError(\"%s doesn't support sample_weight.\"\r\n",
        "                             % self.base_estimator_.__class__.__name__)\r\n",
        "\r\n",
        "    def _boost(self, iboost, X, X_test, y, sample_weight, random_state):\r\n",
        "        if self.algorithm == 'SAMME.R':\r\n",
        "       #     print(\"_9_1_\")\r\n",
        "            return self._boost_real(iboost, X, X_test, y, sample_weight, random_state)\r\n",
        "\r\n",
        "        else:  # elif self.algorithm == \"SAMME\":\r\n",
        "        #    print(\"_9_2_\")\r\n",
        "            return self._boost_discrete(iboost, X, X_test, y, sample_weight,\r\n",
        "                                        random_state)\r\n",
        "\r\n",
        "    def _boost_real(self, iboost, X, X_test, y, sample_weight, random_state):\r\n",
        "        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\r\n",
        "        estimator = self._make_estimator(random_state=random_state)\r\n",
        "\r\n",
        "        estimator.fit(X, y, sample_weight=sample_weight)\r\n",
        "\r\n",
        "        y_predict_proba = estimator.predict_proba(X)\r\n",
        "\r\n",
        "        if iboost == 0:\r\n",
        "            self.classes_ = getattr(estimator, 'classes_', None)\r\n",
        "            self.n_classes_ = len(self.classes_)\r\n",
        "\r\n",
        "        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\r\n",
        "                                       axis=0)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_[:, np.newaxis]\r\n",
        "\r\n",
        "        if self.algorithm == 'SAMME.R':#\r\n",
        "            preds = super()._samme_proba(estimator, n_classes, X_test)\r\n",
        "            self.testpredictions.append(preds) #\r\n",
        "            # The weights are all 1. for SAMME.R\r\n",
        "            # pred = sum(_samme_proba(estimator, n_classes, X)\r\n",
        "            #             for estimator in self.estimators_)\r\n",
        "        else:  # self.algorithm == \"SAMME\" #\r\n",
        "            self.testpredictions.append(estimator.predict(X_test)) #\r\n",
        "            # pred = sum((estimator.predict(X) == classes).T * w\r\n",
        "            #             for estimator, w in zip(self.estimators_,\r\n",
        "            #                                     self.estimator_weights_))\r\n",
        "            \r\n",
        "        # Instances incorrectly classified\r\n",
        "        incorrect = y_predict != y\r\n",
        "\r\n",
        "        # Error fraction\r\n",
        "        estimator_error = np.mean(\r\n",
        "            np.average(incorrect, weights=sample_weight, axis=0))\r\n",
        "\r\n",
        "        # Stop if classification is perfect\r\n",
        "        if estimator_error <= 0:\r\n",
        "            return sample_weight, 1., 0.\r\n",
        "\r\n",
        "        # Construct y coding as described in Zhu et al [2]:\r\n",
        "        #\r\n",
        "        #    y_k = 1 if c == k else -1 / (K - 1)\r\n",
        "        #\r\n",
        "        # where K == n_classes_ and c, k in [0, K) are indices along the second\r\n",
        "        # axis of the y coding with c being the index corresponding to the true\r\n",
        "        # class label.\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_\r\n",
        "        y_codes = np.array([-1. / (n_classes - 1), 1.])\r\n",
        "        y_coding = y_codes.take(classes == y[:, np.newaxis])\r\n",
        "\r\n",
        "        # Displace zero probabilities so the log is defined.\r\n",
        "        # Also fix negative elements which may occur with\r\n",
        "        # negative sample weights.\r\n",
        "        proba = y_predict_proba  # alias for readability\r\n",
        "        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\r\n",
        "\r\n",
        "        # Boost weight using multi-class AdaBoost SAMME.R alg\r\n",
        "        estimator_weight = (-1. * self.learning_rate\r\n",
        "                            * ((n_classes - 1.) / n_classes)\r\n",
        "                            * xlogy(y_coding, y_predict_proba).sum(axis=1))\r\n",
        "\r\n",
        "        # Only boost the weights if it will fit again\r\n",
        "        if not iboost == self.n_estimators - 1:\r\n",
        "            # Only boost positive weights\r\n",
        "            sample_weight *= np.exp(estimator_weight *\r\n",
        "                                    ((sample_weight > 0) |\r\n",
        "                                     (estimator_weight < 0)))\r\n",
        "\r\n",
        "        return sample_weight, 1., estimator_error\r\n",
        "\r\n",
        "    def _boost_discrete(self, iboost, X, X_test, y, sample_weight, random_state):\r\n",
        "        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\r\n",
        "        estimator = self._make_estimator(random_state=random_state)\r\n",
        "\r\n",
        "        estimator.fit(X, y, sample_weight=sample_weight)\r\n",
        "\r\n",
        "        y_predict = estimator.predict(X)\r\n",
        "\r\n",
        "        test_predict_proba = estimator.predict_proba(X_test) #\r\n",
        "        test_predict = self.classes_.take(np.argmax(test_predict_proba, axis=1),\r\n",
        "                                       axis=0) #\r\n",
        "        self.predictions.append(test_predict)#\r\n",
        "\r\n",
        "\r\n",
        "        if iboost == 0:\r\n",
        "            self.classes_ = getattr(estimator, 'classes_', None)\r\n",
        "            self.n_classes_ = len(self.classes_)\r\n",
        "\r\n",
        "        # Instances incorrectly classified\r\n",
        "        incorrect = y_predict != y\r\n",
        "\r\n",
        "        # Error fraction\r\n",
        "        estimator_error = np.mean(\r\n",
        "            np.average(incorrect, weights=sample_weight, axis=0))\r\n",
        "\r\n",
        "        # Stop if classification is perfect\r\n",
        "        if estimator_error <= 0:\r\n",
        "            return sample_weight, 1., 0.\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "\r\n",
        "        # Stop if the error is at least as bad as random guessing\r\n",
        "        if estimator_error >= 1. - (1. / n_classes):\r\n",
        "            self.estimators_.pop(-1)\r\n",
        "            if len(self.estimators_) == 0:\r\n",
        "                raise ValueError('BaseClassifier in AdaBoostClassifier '\r\n",
        "                                 'ensemble is worse than random, ensemble '\r\n",
        "                                 'can not be fit.')\r\n",
        "            return None, None, None\r\n",
        "\r\n",
        "        # Boost weight using multi-class AdaBoost SAMME alg\r\n",
        "        estimator_weight = self.learning_rate * (\r\n",
        "            np.log((1. - estimator_error) / estimator_error) +\r\n",
        "            np.log(n_classes - 1.))\r\n",
        "\r\n",
        "        # Only boost the weights if I will fit again\r\n",
        "        if not iboost == self.n_estimators - 1:\r\n",
        "            # Only boost positive weights\r\n",
        "            sample_weight *= np.exp(estimator_weight * incorrect *\r\n",
        "                                    (sample_weight > 0))\r\n",
        "\r\n",
        "        return sample_weight, estimator_weight, estimator_error\r\n",
        "\r\n",
        "    def predict(self, X):\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        pred = self.decision_function(X)\r\n",
        "\r\n",
        "        if self.n_classes_ == 2:\r\n",
        "            return self.classes_.take(pred > 0, axis=0)\r\n",
        "\r\n",
        "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\r\n",
        "    \r\n",
        "    def predict(self): #\r\n",
        "        # X = self._check_X(X)\r\n",
        "\r\n",
        "        pred = self.decision_function_test()\r\n",
        "\r\n",
        "        if self.n_classes_ == 2:\r\n",
        "            return self.classes_.take(pred > 0, axis=0)\r\n",
        "\r\n",
        "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\r\n",
        "\r\n",
        "    def staged_predict(self, X):\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_\r\n",
        "\r\n",
        "        if n_classes == 2:\r\n",
        "            for pred in self.staged_decision_function(X):\r\n",
        "                yield np.array(classes.take(pred > 0, axis=0))\r\n",
        "\r\n",
        "        else:\r\n",
        "            for pred in self.staged_decision_function(X):\r\n",
        "                yield np.array(classes.take(\r\n",
        "                    np.argmax(pred, axis=1), axis=0))\r\n",
        "                \r\n",
        "    def decision_function_test(self): #\r\n",
        "      check_is_fitted(self)\r\n",
        "      # X = self._check_X(X)\r\n",
        "\r\n",
        "      n_classes = self.n_classes_\r\n",
        "      classes = self.classes_[:, np.newaxis]\r\n",
        "\r\n",
        "      if self.algorithm == 'SAMME.R':\r\n",
        "          pred = sum(predicts for predicts in self.testpredictions)\r\n",
        "      else:\r\n",
        "          pred = sum( (predicts == classes).T * w \r\n",
        "                     for predicts, w in zip(self.testpredictions, \r\n",
        "                                            self.estimator_weights_))\r\n",
        "      # if self.algorithm == 'SAMME.R':\r\n",
        "      #     # The weights are all 1. for SAMME.R\r\n",
        "      #     pred = sum(_samme_proba(estimator, n_classes, X)\r\n",
        "      #                 for estimator in self.estimators_)\r\n",
        "      # else:  # self.algorithm == \"SAMME\"\r\n",
        "      #     pred = sum((estimator.predict(X) == classes).T * w\r\n",
        "      #                 for estimator, w in zip(self.estimators_,\r\n",
        "      #                                         self.estimator_weights_))\r\n",
        "\r\n",
        "      pred /= self.estimator_weights_.sum()\r\n",
        "      if n_classes == 2:\r\n",
        "          pred[:, 0] *= -1\r\n",
        "          return pred.sum(axis=1)\r\n",
        "      return pred\r\n",
        "\r\n",
        "\r\n",
        "    def decision_function(self, X):\r\n",
        "        check_is_fitted(self)\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_[:, np.newaxis]\r\n",
        "\r\n",
        "        if self.algorithm == 'SAMME.R':\r\n",
        "            # The weights are all 1. for SAMME.R\r\n",
        "            pred = sum(_samme_proba(estimator, n_classes, X)\r\n",
        "                       for estimator in self.estimators_)\r\n",
        "        else:  # self.algorithm == \"SAMME\"\r\n",
        "            pred = sum((estimator.predict(X) == classes).T * w\r\n",
        "                       for estimator, w in zip(self.estimators_,\r\n",
        "                                               self.estimator_weights_))\r\n",
        "\r\n",
        "        pred /= self.estimator_weights_.sum()\r\n",
        "        if n_classes == 2:\r\n",
        "            pred[:, 0] *= -1\r\n",
        "            return pred.sum(axis=1)\r\n",
        "        return pred\r\n",
        "\r\n",
        "    def staged_decision_function(self, X):\r\n",
        "        check_is_fitted(self)\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_[:, np.newaxis]\r\n",
        "        pred = None\r\n",
        "        norm = 0.\r\n",
        "\r\n",
        "        for weight, estimator in zip(self.estimator_weights_,\r\n",
        "                                     self.estimators_):\r\n",
        "            norm += weight\r\n",
        "\r\n",
        "            if self.algorithm == 'SAMME.R':\r\n",
        "                # The weights are all 1. for SAMME.R\r\n",
        "                current_pred = _samme_proba(estimator, n_classes, X)\r\n",
        "            else:  # elif self.algorithm == \"SAMME\":\r\n",
        "                current_pred = estimator.predict(X)\r\n",
        "                current_pred = (current_pred == classes).T * weight\r\n",
        "\r\n",
        "            if pred is None:\r\n",
        "                pred = current_pred\r\n",
        "            else:\r\n",
        "                pred += current_pred\r\n",
        "\r\n",
        "            if n_classes == 2:\r\n",
        "                tmp_pred = np.copy(pred)\r\n",
        "                tmp_pred[:, 0] *= -1\r\n",
        "                yield (tmp_pred / norm).sum(axis=1)\r\n",
        "            else:\r\n",
        "                yield pred / norm\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def _compute_proba_from_decision(decision, n_classes):\r\n",
        "        if n_classes == 2:\r\n",
        "            decision = np.vstack([-decision, decision]).T / 2\r\n",
        "        else:\r\n",
        "            decision /= (n_classes - 1)\r\n",
        "        return softmax(decision, copy=False)\r\n",
        "\r\n",
        "    def predict_proba(self, X):\r\n",
        "        check_is_fitted(self)\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "\r\n",
        "        if n_classes == 1:\r\n",
        "            return np.ones((_num_samples(X), 1))\r\n",
        "\r\n",
        "        decision = self.decision_function(X)\r\n",
        "        return self._compute_proba_from_decision(decision, n_classes)\r\n",
        "\r\n",
        "    def staged_predict_proba(self, X):\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "\r\n",
        "        for decision in self.staged_decision_function(X):\r\n",
        "            yield self._compute_proba_from_decision(decision, n_classes)\r\n",
        "\r\n",
        "    def predict_log_proba(self, X):\r\n",
        "        X = self._check_X(X)\r\n",
        "        return np.log(self.predict_proba(X))\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw1YaJWEsciw"
      },
      "source": [
        "Adaboost\r\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWTgR3HrWXLv"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\r\n",
        "from sklearn.model_selection import RepeatedKFold\r\n",
        "import warnings\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "foz=4\r\n",
        "imp_feature_size=1\r\n",
        "n_estimators = 10\r\n",
        "#Table\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "pt_adaboost = PrettyTable()\r\n",
        "\r\n",
        "pt_adaboost.field_names = [\"Dataset\", \"Imp_Adaboost\", \"Adaboost\"]\r\n",
        "\r\n",
        "foldPreds_AB = {}\r\n",
        "K =0\r\n",
        "for ds in datasets:\r\n",
        "  K+=1\r\n",
        "  rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=2652124)\r\n",
        "  if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "    df = datasets[ds]\r\n",
        "    X = df['text']\r\n",
        "    Y = df['label']\r\n",
        "  else:\r\n",
        "    X,Y = datasets[ds]\r\n",
        "    Y = Y.str.decode(\"utf-8\")\r\n",
        "    \r\n",
        "  # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\r\n",
        "  print(ds)\r\n",
        "  accuracies = []\r\n",
        "  accuracies_imp = []  \r\n",
        "  for train_index, test_index in rkf.split(X):\r\n",
        "\r\n",
        "    X_train = X.iloc[train_index]\r\n",
        "    X_test = X.iloc[test_index]\r\n",
        "    Y_train = Y.iloc[train_index]\r\n",
        "    Y_test = Y.iloc[test_index]\r\n",
        "\r\n",
        "    if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "      vectorizer = CountVectorizer(analyzer=preprocess)\r\n",
        "      vectorizer.fit(X_train)\r\n",
        "\r\n",
        "      X_train = vectorizer.transform(X_train).toarray()\r\n",
        "      X_test = vectorizer.transform(X_test).toarray()\r\n",
        "      X_train = pd.DataFrame(X_train)\r\n",
        "      X_test = pd.DataFrame(X_test)\r\n",
        "\r\n",
        "    d = len(X_train.columns)\r\n",
        "    num_class = len(Y_train.value_counts())\r\n",
        "    \r\n",
        "    ab_clf = AdaBoostClassifier(n_estimators=10)\r\n",
        "    ab_clf.fit(X=X_train,y=Y_train)\r\n",
        "    y_preds = ab_clf.predict(X_test)\r\n",
        "\r\n",
        "    # print(\"-----------------------------------------------------------\")\r\n",
        "    ab_clf_imp = AdaBoostClassifier_IMP(n_estimators=10)\r\n",
        "    ab_clf_imp.fit(X=X_train, X_test=X_test , y=Y_train)\r\n",
        "    y_preds_imp = ab_clf_imp.predict()\r\n",
        "\r\n",
        "    # print(accuracy_score(Y_test,y_preds))\r\n",
        "    # print(accuracy_score(Y_test,y_preds_imp))\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    # print(\"--------------  {}  ----------------\".format(ds.upper()))\r\n",
        "    accuracies.append(accuracy_score(Y_test,y_preds))\r\n",
        "    accuracies_imp.append(accuracy_score(Y_test,y_preds_imp))\r\n",
        "    # print(\" \")\r\n",
        "    # print(\" \")\r\n",
        "    pt_adaboost.add_row((ds, \"%.4f\" % accuracy_score(Y_test,y_preds_imp), \"%.4f\" % accuracy_score(Y_test,y_preds)))\r\n",
        "\r\n",
        "  foldPreds_AB[ds] = accuracies\r\n",
        "  foldPreds_AB[ds+\"_imp\"] = accuracies_imp\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr-LjV01W0v8"
      },
      "source": [
        "print(pt_adaboost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9dXJSK_vTjY"
      },
      "source": [
        "np.save(ds_folderpath + \"/\" + 'adaboost.npy',foldPreds_AB ) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjkOqaARshcG"
      },
      "source": [
        "Sonuçlar\r\n",
        "-----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVJWXRV9O43j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234b4b37-6328-4d63-967a-f926dd65668b"
      },
      "source": [
        "foldPreds_Bagging = np.load(\"gdrive/MyDrive/ImprovedSpace/\" + 'bagging.npy',allow_pickle='TRUE').item()\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "result_Bagging = PrettyTable()\r\n",
        "\r\n",
        "result_Bagging.field_names = [\"Dataset\",\"Win_Improved\",\"Loss_Improved\" ,\"Imp_Bagging\", \"Bagging\"]\r\n",
        "for ds in datasets:\r\n",
        "  win = np.array(foldPreds_Bagging[ds]) < np.array(foldPreds_Bagging[ds+'_imp'])\r\n",
        "  loss = np.array(foldPreds_Bagging[ds]) > np.array(foldPreds_Bagging[ds+'_imp'])\r\n",
        "  cnt_win= np.count_nonzero(win)\r\n",
        "  cnt_loss= np.count_nonzero(loss)\r\n",
        "  ave= np.average(foldPreds_Bagging[ds])\r\n",
        "  ave_imp= np.average(foldPreds_Bagging[ds+\"_imp\"])\r\n",
        "  result_Bagging.add_row((ds, cnt_win,cnt_loss, \"%.4f\" % ave_imp, \"%.4f\" % ave))\r\n",
        "\r\n",
        "print(result_Bagging)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------+---------------+-------------+---------+\n",
            "|    Dataset     | Win_Improved | Loss_Improved | Imp_Bagging | Bagging |\n",
            "+----------------+--------------+---------------+-------------+---------+\n",
            "|   ionosphere   |      8       |       1       |    0.9293   |  0.9094 |\n",
            "|    diabetes    |      7       |       3       |    0.7456   |  0.7385 |\n",
            "|     glass      |      3       |       7       |    0.6665   |  0.7091 |\n",
            "|    abalone     |      8       |       2       |    0.2350   |  0.2293 |\n",
            "| heart-statlog  |      8       |       2       |    0.8074   |  0.7837 |\n",
            "|     colic      |      5       |       5       |    0.8359   |  0.8315 |\n",
            "|    credit-g    |      4       |       6       |    0.7308   |  0.7340 |\n",
            "| balance-scale  |      10      |       0       |    0.9274   |  0.8089 |\n",
            "|     autos      |      3       |       5       |    0.6634   |  0.6644 |\n",
            "|    credit-a    |      4       |       5       |    0.8470   |  0.8490 |\n",
            "|    breast-w    |      9       |       1       |    0.9637   |  0.9551 |\n",
            "| breast-cancer  |      7       |       3       |    0.6979   |  0.6916 |\n",
            "|   audiology    |      7       |       2       |    0.8758   |  0.8533 |\n",
            "|     lymph      |      5       |       4       |    0.8183   |  0.8070 |\n",
            "|   hepatitis    |      7       |       2       |    0.8309   |  0.8038 |\n",
            "|     labor      |      4       |       3       |    0.8738   |  0.8667 |\n",
            "|      iris      |      5       |       1       |    0.9640   |  0.9467 |\n",
            "|     letter     |      5       |       5       |    0.9089   |  0.9087 |\n",
            "|     vowel      |      10      |       0       |    0.8396   |  0.8109 |\n",
            "|    soybean     |      7       |       3       |    0.9129   |  0.9123 |\n",
            "|    segment     |      10      |       0       |    0.9706   |  0.9591 |\n",
            "|      vote      |      5       |       3       |    0.9540   |  0.9540 |\n",
            "|     sonar      |      8       |       1       |    0.7952   |  0.7385 |\n",
            "|      zoo       |      3       |       0       |    0.9976   |  0.9857 |\n",
            "|    vehicle     |      10      |       0       |    0.7430   |  0.7076 |\n",
            "|    waveform    |      10      |       0       |    0.8297   |  0.8056 |\n",
            "| primary-tumor  |      5       |       4       |    0.4497   |  0.4285 |\n",
            "|     splice     |      7       |       2       |    0.9461   |  0.9436 |\n",
            "|    ringnorm    |      10      |       0       |    0.9775   |  0.9338 |\n",
            "|     col10      |      6       |       4       |    0.7763   |  0.7754 |\n",
            "|      cmc       |      8       |       2       |    0.5256   |  0.5120 |\n",
            "|    column3C    |      6       |       3       |    0.8277   |  0.8181 |\n",
            "|     ecoli      |      8       |       2       |    0.8416   |  0.8269 |\n",
            "|      monk      |      5       |       5       |    0.8951   |  0.9049 |\n",
            "|  transfusion   |      7       |       3       |    0.7398   |  0.7278 |\n",
            "| wineCultivars  |      6       |       2       |    0.9621   |  0.9333 |\n",
            "|   hillValley   |      10      |       0       |    0.9749   |  0.5198 |\n",
            "| movementLibras |      10      |       0       |    0.7350   |  0.6433 |\n",
            "|    spambase    |      9       |       0       |    0.9257   |  0.9162 |\n",
            "|     yeast      |      6       |       3       |    0.5652   |  0.5607 |\n",
            "|  iphonetweets  |      6       |       4       |    0.7508   |  0.7459 |\n",
            "|  hobbittweets  |      3       |       1       |    0.9330   |  0.9310 |\n",
            "+----------------+--------------+---------------+-------------+---------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ58Bd0KqNAb",
        "outputId": "335a5b39-c52f-4e3c-a3b7-028b99cab0c2"
      },
      "source": [
        "foldPreds_RF = np.load(\"gdrive/MyDrive/ImprovedSpace/\" + 'RF.npy',allow_pickle='TRUE').item()\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "result_RF = PrettyTable()\r\n",
        "\r\n",
        "result_RF.field_names = [\"Dataset\",\"Win_Improved\",\"Loss_Improved\" ,\"Imp_RF\", \"RF\"]\r\n",
        "for ds in datasets:\r\n",
        "  win = np.array(foldPreds_RF[ds]) < np.array(foldPreds_RF[ds+'_imp'])\r\n",
        "  loss = np.array(foldPreds_RF[ds]) > np.array(foldPreds_RF[ds+'_imp'])\r\n",
        "  cnt_win= np.count_nonzero(win)\r\n",
        "  cnt_loss= np.count_nonzero(loss)\r\n",
        "  ave= np.average(foldPreds_RF[ds])\r\n",
        "  ave_imp= np.average(foldPreds_RF[ds+\"_imp\"])\r\n",
        "  result_RF.add_row((ds, cnt_win, cnt_loss, \"%.4f\" % ave_imp, \"%.4f\" % ave))\r\n",
        "\r\n",
        "print(result_RF)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------+---------------+--------+--------+\n",
            "|    Dataset     | Win_Improved | Loss_Improved | Imp_RF |   RF   |\n",
            "+----------------+--------------+---------------+--------+--------+\n",
            "|   ionosphere   |      7       |       2       | 0.9310 | 0.9168 |\n",
            "|    diabetes    |      9       |       1       | 0.7440 | 0.7320 |\n",
            "|     glass      |      6       |       3       | 0.7045 | 0.7014 |\n",
            "|    abalone     |      5       |       5       | 0.2276 | 0.2270 |\n",
            "| heart-statlog  |      7       |       3       | 0.8037 | 0.7874 |\n",
            "|     colic      |      1       |       6       | 0.8082 | 0.8158 |\n",
            "|    credit-g    |      7       |       3       | 0.7378 | 0.7282 |\n",
            "| balance-scale  |      10      |       0       | 0.9261 | 0.8038 |\n",
            "|     autos      |      4       |       6       | 0.6614 | 0.6683 |\n",
            "|    credit-a    |      4       |       5       | 0.8583 | 0.8606 |\n",
            "|    breast-w    |      9       |       1       | 0.9642 | 0.9548 |\n",
            "| breast-cancer  |      4       |       6       | 0.7007 | 0.7098 |\n",
            "|   audiology    |      7       |       1       | 0.8580 | 0.8201 |\n",
            "|     lymph      |      4       |       6       | 0.8394 | 0.8310 |\n",
            "|   hepatitis    |      6       |       2       | 0.8309 | 0.8194 |\n",
            "|     labor      |      3       |       3       | 0.8950 | 0.9021 |\n",
            "|      iris      |      4       |       4       | 0.9600 | 0.9560 |\n",
            "|     letter     |      1       |       9       | 0.9089 | 0.9159 |\n",
            "|     vowel      |      10      |       0       | 0.8564 | 0.8251 |\n",
            "|    soybean     |      5       |       5       | 0.9099 | 0.9120 |\n",
            "|    segment     |      8       |       2       | 0.9695 | 0.9669 |\n",
            "|      vote      |      3       |       5       | 0.9545 | 0.9568 |\n",
            "|     sonar      |      8       |       2       | 0.7865 | 0.7519 |\n",
            "|      zoo       |      0       |       3       | 0.9929 | 1.0000 |\n",
            "|    vehicle     |      9       |       1       | 0.7487 | 0.7125 |\n",
            "|    waveform    |      10      |       0       | 0.8293 | 0.8097 |\n",
            "| primary-tumor  |      8       |       2       | 0.4497 | 0.4278 |\n",
            "|     splice     |      9       |       1       | 0.9328 | 0.9206 |\n",
            "|    ringnorm    |      10      |       0       | 0.9779 | 0.9339 |\n",
            "|     col10      |      5       |       5       | 0.7805 | 0.7796 |\n",
            "|      cmc       |      6       |       4       | 0.5195 | 0.5130 |\n",
            "|    column3C    |      7       |       3       | 0.8439 | 0.8194 |\n",
            "|     ecoli      |      9       |       1       | 0.8594 | 0.8349 |\n",
            "|      monk      |      6       |       3       | 0.9082 | 0.9016 |\n",
            "|  transfusion   |      5       |       3       | 0.7401 | 0.7332 |\n",
            "| wineCultivars  |      8       |       2       | 0.9634 | 0.9477 |\n",
            "|   hillValley   |      10      |       0       | 0.9739 | 0.5228 |\n",
            "| movementLibras |      7       |       2       | 0.6889 | 0.6400 |\n",
            "|    spambase    |      3       |       5       | 0.9227 | 0.9228 |\n",
            "|     yeast      |      6       |       4       | 0.5711 | 0.5623 |\n",
            "|  iphonetweets  |      5       |       4       | 0.7590 | 0.7575 |\n",
            "|  hobbittweets  |      3       |       7       | 0.9199 | 0.9272 |\n",
            "+----------------+--------------+---------------+--------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KunTufROqQa0",
        "outputId": "3c4af991-af58-4deb-f7d6-7544fe4b9b28"
      },
      "source": [
        "foldPreds_AB = np.load(\"gdrive/MyDrive/ImprovedSpace/\" + 'adaboost.npy',allow_pickle='TRUE').item()\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "result_AB = PrettyTable()\r\n",
        "\r\n",
        "result_AB.field_names = [\"Dataset\",\"Win_Improved\",\"Loss_Improved\" ,\"Imp_Adaboost\", \"Adaboost\"]\r\n",
        "for ds in datasets:\r\n",
        "  win = np.array(foldPreds_AB[ds]) < np.array(foldPreds_AB[ds+'_imp'])\r\n",
        "  loss = np.array(foldPreds_AB[ds]) > np.array(foldPreds_AB[ds+'_imp'])\r\n",
        "  cnt_win= np.count_nonzero(win)\r\n",
        "  cnt_loss= np.count_nonzero(loss)\r\n",
        "  ave= np.average(foldPreds_AB[ds])\r\n",
        "  ave_imp= np.average(foldPreds_AB[ds+\"_imp\"])\r\n",
        "  result_AB.add_row((ds, cnt_win,cnt_loss, \"%.4f\" % ave_imp, \"%.4f\" % ave))\r\n",
        "\r\n",
        "print(result_AB)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------+---------------+--------------+----------+\n",
            "|    Dataset     | Win_Improved | Loss_Improved | Imp_Adaboost | Adaboost |\n",
            "+----------------+--------------+---------------+--------------+----------+\n",
            "|   ionosphere   |      6       |       1       |    0.9168    |  0.9048  |\n",
            "|    diabetes    |      7       |       3       |    0.7495    |  0.7375  |\n",
            "|     glass      |      9       |       1       |    0.5005    |  0.4294  |\n",
            "|    abalone     |      6       |       4       |    0.2199    |  0.2225  |\n",
            "| heart-statlog  |      4       |       5       |    0.7859    |  0.7926  |\n",
            "|     colic      |      2       |       7       |    0.7804    |  0.7880  |\n",
            "|    credit-g    |      3       |       7       |    0.7070    |  0.7140  |\n",
            "| balance-scale  |      6       |       4       |    0.8220    |  0.8518  |\n",
            "|     autos      |      7       |       3       |    0.4505    |  0.4267  |\n",
            "|    credit-a    |      5       |       5       |    0.8414    |  0.8397  |\n",
            "|    breast-w    |      8       |       2       |    0.9542    |  0.9462  |\n",
            "| breast-cancer  |      1       |       7       |    0.6650    |  0.6881  |\n",
            "|   audiology    |      2       |       5       |    0.7623    |  0.8061  |\n",
            "|     lymph      |      2       |       4       |    0.7845    |  0.8028  |\n",
            "|   hepatitis    |      5       |       5       |    0.8141    |  0.8220  |\n",
            "|     labor      |      2       |       7       |    0.8140    |  0.8700  |\n",
            "|      iris      |      4       |       2       |    0.9547    |  0.9493  |\n",
            "|     letter     |      7       |       3       |    0.2539    |  0.2304  |\n",
            "|     vowel      |      8       |       1       |    0.2117    |  0.1723  |\n",
            "|    soybean     |      8       |       2       |    0.3902    |  0.3404  |\n",
            "|    segment     |      7       |       0       |    0.4755    |  0.4338  |\n",
            "|      vote      |      5       |       3       |    0.9513    |  0.9536  |\n",
            "|     sonar      |      7       |       3       |    0.7692    |  0.7327  |\n",
            "|      zoo       |      2       |       0       |    0.9905    |  0.9833  |\n",
            "|    vehicle     |      10      |       0       |    0.6293    |  0.5340  |\n",
            "|    waveform    |      8       |       1       |    0.7966    |  0.7801  |\n",
            "| primary-tumor  |      4       |       5       |    0.3026    |  0.3252  |\n",
            "|     splice     |      4       |       6       |    0.9170    |  0.9295  |\n",
            "|    ringnorm    |      10      |       0       |    0.9695    |  0.8597  |\n",
            "|     col10      |      0       |       8       |    0.6604    |  0.6703  |\n",
            "|      cmc       |      4       |       6       |    0.5295    |  0.5329  |\n",
            "|    column3C    |      8       |       0       |    0.7155    |  0.6039  |\n",
            "|     ecoli      |      3       |       5       |    0.6713    |  0.6728  |\n",
            "|      monk      |      2       |       6       |    0.8803    |  0.8770  |\n",
            "|  transfusion   |      6       |       3       |    0.7775    |  0.7695  |\n",
            "| wineCultivars  |      10      |       0       |    0.9490    |  0.8587  |\n",
            "|   hillValley   |      10      |       0       |    0.9726    |  0.5017  |\n",
            "| movementLibras |      9       |       1       |    0.1817    |  0.1128  |\n",
            "|    spambase    |      4       |       6       |    0.8952    |  0.8949  |\n",
            "|     yeast      |      3       |       4       |    0.4331    |  0.4354  |\n",
            "|  iphonetweets  |      2       |       2       |    0.7098    |  0.7105  |\n",
            "|  hobbittweets  |      1       |       1       |    0.9448    |  0.9444  |\n",
            "+----------------+--------------+---------------+--------------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}