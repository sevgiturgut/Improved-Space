{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KollektifProje_Bagging_RF_AB_CV",
      "provenance": [],
      "collapsed_sections": [
        "_vgNW6ZBvEJM"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7ftrIKBvEQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b54e6c-60d2-481e-abb9-cf1b551ded07"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pylab import savefig\r\n",
        "from scipy.io import arff\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "import ntpath\r\n",
        "import glob\r\n",
        "import os\r\n",
        "import math\r\n",
        "# !pip install liac-arff\r\n",
        "#import arff\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from random import randrange\r\n",
        "from itertools import combinations as comb\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.ensemble import VotingClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "import string\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouZ_LPRzlfD1"
      },
      "source": [
        "def preprocess(listoftext):\r\n",
        "    customstopwords = nltk.corpus.stopwords.words('english')\r\n",
        "    customstopwords.append('rt')\r\n",
        "    emojipattern = re.compile(\"[\"\r\n",
        "                              u\"\\U0001F600-\\U0001F64F\"\r\n",
        "                              u\"\\U0001F300-\\U0001F5FF\"\r\n",
        "                              u\"\\U0001F680-\\U0001F6FF\"\r\n",
        "                              u\"\\U0001F1E0-\\U0001F1FF\"\r\n",
        "                              u\"\\U00000430-\\U00000648\"\r\n",
        "                              u\"\\U0000263a-\\U0000fe0f\"\r\n",
        "                              u\"\\U0000201c-\\U0000201d\"\r\n",
        "                              u\"\\U00002000-\\U000020e3\"\r\n",
        "                              u\"\\U0000064a-\\U0000064d\"\r\n",
        "                              u\"\\U0000ff00-\\U0000ff09\"\r\n",
        "                              u\"\\U000fe520-\\U000fe529\"\r\n",
        "                              u\"\\U0000221b-\\U0000221e\"\r\n",
        "                              u\"\\U000feb90-\\U000feb99\"\r\n",
        "                              u\"\\U00001d20-\\U00001d4c\"\r\n",
        "                              u\"\\U00000080-\\U000000FF\"\r\n",
        "                              u\"\\U00000100-\\U00000139\"\r\n",
        "                              u\"\\U0000FF80-\\U0001007F\"\"]+\", flags=re.UNICODE)\r\n",
        "    httppattern = re.compile('http?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\r\n",
        "\r\n",
        "    punctuationremoved = [char for char in listoftext if char not in  string.punctuation]\r\n",
        "    punctuationremoved = ''.join(punctuationremoved)\r\n",
        "\r\n",
        "    advancedremoved = emojipattern.sub(r'', punctuationremoved)\r\n",
        "    advancedremoved = httppattern.sub(r'', advancedremoved)\r\n",
        "\r\n",
        "    stopwordsremoved = [word for word in advancedremoved.split() if word.lower() not in customstopwords]\r\n",
        "    return stopwordsremoved"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BybuTagCAUwV"
      },
      "source": [
        "# <h1> Datasets</h1>\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suGHsjVS10uc"
      },
      "source": [
        "ds_folderpath = \"gdrive/MyDrive/ImprovedSpace/arfffiles/\"\n",
        "ds_files = glob.glob(ds_folderpath + \"/*.arff\")\n",
        "datasets = {} # dictionary\n",
        "for file_path in ds_files:\n",
        "  filename = os.path.splitext(ntpath.basename(file_path))[0]\n",
        "  if filename != \"cnae\" and filename != \"cnae_utf\": # cnae veriseti utf olmadığı için, eskisi yerine aynı içerikte, utf encoded hali kaydedidi: cnae_utf.arff \n",
        "    filedata, meta = arff.loadarff(file_path)\n",
        "    attrNames = meta.names()\n",
        "    classname = attrNames[-1]\n",
        "    attrnames = attrNames[0:-1]\n",
        "    \n",
        "    data = pd.DataFrame(filedata)\n",
        "    samples = data[attrnames]\n",
        "    labels = data[classname]\n",
        "    datasets[filename] = (samples, labels)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8DFFT6nlpaK"
      },
      "source": [
        "ds_folderpath = \"gdrive/MyDrive/ImprovedSpace/\"\r\n",
        "ds_files = glob.glob(ds_folderpath + \"/\")\r\n",
        "\r\n",
        "datasets[\"iphonetweets\"] = pd.read_csv(ds_folderpath+'iphonetweets.csv', sep=',', names=['text', 'label'], usecols=['text', 'label'])\r\n",
        "datasets[\"hobbittweets\"] = pd.read_csv(ds_folderpath+'Hobbittweets.csv', sep=',', names=['text', 'label'],usecols=['text', 'label'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Z4J48tA97u"
      },
      "source": [
        "Functions \r\n",
        "----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-F5iBZ08nuK"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def x2fx(x, model='quadratic'):\r\n",
        "    linear = np.c_[np.ones(x.shape[0]), x]\r\n",
        "    if model == 'linear':\r\n",
        "        return linear\r\n",
        "    if model == 'purequadratic':\r\n",
        "        return np.c_[linear, x**2]\r\n",
        "    interaction = np.array([x[:,i]*x[:,j] for i, j in comb(range(x.shape[1]), 2)]).T\r\n",
        "    if model == 'interaction':\r\n",
        "        return np.c_[linear, interaction]\r\n",
        "    if model == 'quadratic':\r\n",
        "        return np.c_[linear, interaction, x**2]\r\n",
        "\r\n",
        "def generate_imp_space(X_train, Y_train, X_test, imp_feature_size, foz):\r\n",
        "  imp_train_data = X_train.values\r\n",
        "  imp_test_data = X_test.values\r\n",
        "  d = len(X_train.columns)\r\n",
        "  # print(\"____1_____\")\r\n",
        "  for i in range(0,imp_feature_size*foz):\r\n",
        "    Xindis = np.random.permutation(d)\r\n",
        "    for j in range(0,d-(foz-1),foz):  #d/foz kadar doner\r\n",
        "      sX = np.random.permutation(num_class)\r\n",
        "      s1 = sX[0]\r\n",
        "      # print(\"____2_____\")\r\n",
        "      s1data = X_train[X_train.index.isin(Y_train[Y_train == str(s1)].index)]\r\n",
        "      s2data = X_train[~X_train.index.isin(Y_train[Y_train == str(s1)].index)]\r\n",
        "      s1data = s1data.iloc[:,Xindis[j:j+(foz)]]\r\n",
        "      s2data = s2data.iloc[:,Xindis[j:j+(foz)]] # s1 vs all other classes, #foz feature\r\n",
        "      # print(\"____3_____\")\r\n",
        "      s1label = np.ones((s1data.values.shape[0],1),dtype=int)\r\n",
        "      s2label = -1*np.ones((s2data.values.shape[0],1),dtype=int)\r\n",
        "      Wdata = np.concatenate((s1data,s2data))\r\n",
        "      # print(\"____4_____\")\r\n",
        "      \r\n",
        "      Wdata = x2fx(Wdata)\r\n",
        "      Wlabel = np.concatenate((s1label,s2label))\r\n",
        "      W = np.matmul(np.matmul(np.linalg.pinv(np.matmul(Wdata.T, Wdata)),Wdata.T),Wlabel)\r\n",
        "      \r\n",
        "      WW = x2fx(X_train.iloc[:,Xindis[j:j+(foz)]].values)\r\n",
        "      imp_train_data = np.concatenate((imp_train_data, np.matmul(WW,W)),axis=1)\r\n",
        "      \r\n",
        "      TT = x2fx(X_test.iloc[:,Xindis[j:j+(foz)]].values)\r\n",
        "      imp_test_data = np.concatenate((imp_test_data, np.matmul(TT,W)),axis=1)\r\n",
        "    \r\n",
        "  return imp_train_data,imp_test_data\r\n",
        "\r\n",
        "# Create a random subsample from the dataset with replacement\r\n",
        "\r\n",
        "def subsample(X, X_imp, Y, ratio):\r\n",
        "    xsample = list()\r\n",
        "    ximpsample = list()\r\n",
        "    labels = list()\r\n",
        "    n_sample = round(len(X) * ratio)\r\n",
        "    while len(xsample) < n_sample:\r\n",
        "        index = randrange(len(X))\r\n",
        "        xsample.append(X[index])\r\n",
        "        ximpsample.append(X_imp[index])\r\n",
        "        labels.append(Y[index])\r\n",
        "    return np.array(xsample),np.array(ximpsample),np.array(labels)\r\n",
        "\r\n",
        "\r\n",
        "def MajorityVoting(votes):\r\n",
        "  results = []\r\n",
        "  for i in range(0,votes.shape[1]):\r\n",
        "    values, counts = np.unique(votes[:,i], return_counts=True)\r\n",
        "    results.append(values[np.argmax(counts)])\r\n",
        "  return np.array(results)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-kHn-AKCUa"
      },
      "source": [
        "Random Forest\r\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkOllNmFZ3vw"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\r\n",
        "from sklearn.model_selection import RepeatedKFold\r\n",
        "\r\n",
        "\r\n",
        "foz=4\r\n",
        "imp_feature_size=1\r\n",
        "n_estimators = 10\r\n",
        "#Table\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "pt_RF = PrettyTable()\r\n",
        "\r\n",
        "pt_RF.field_names = [\"Dataset\", \"Imp_RF\", \"RF\"]\r\n",
        "#\r\n",
        "foldPreds_RF = {}\r\n",
        "K =0\r\n",
        "for ds in datasets:\r\n",
        "  K+=1\r\n",
        "  rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=2652124)\r\n",
        "  if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "    df = datasets[ds]\r\n",
        "    X = df['text']\r\n",
        "    Y = df['label']\r\n",
        "  else:\r\n",
        "    X,Y = datasets[ds]\r\n",
        "    Y = Y.str.decode(\"utf-8\")\r\n",
        "    \r\n",
        "  # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\r\n",
        "  print(ds)\r\n",
        "  accuracies = []\r\n",
        "  accuracies_imp = []  \r\n",
        "  for train_index, test_index in rkf.split(X):\r\n",
        "\r\n",
        "    # X_train, X_test = X[train_index], X[test_index]\r\n",
        "    # Y_train, Y_test = Y[train_index], Y[test_index]\r\n",
        "    X_train = X.iloc[train_index]\r\n",
        "    X_test = X.iloc[test_index]\r\n",
        "    Y_train = Y.iloc[train_index]\r\n",
        "    Y_test = Y.iloc[test_index]\r\n",
        "\r\n",
        "    if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "      vectorizer = CountVectorizer(analyzer=preprocess)\r\n",
        "      vectorizer.fit(X_train)\r\n",
        "\r\n",
        "      X_train = vectorizer.transform(X_train).toarray()\r\n",
        "      X_test = vectorizer.transform(X_test).toarray()\r\n",
        "      X_train = pd.DataFrame(X_train)\r\n",
        "      X_test = pd.DataFrame(X_test)\r\n",
        "\r\n",
        "    d = len(X_train.columns)\r\n",
        "    num_class = len(Y_train.value_counts())\r\n",
        "\r\n",
        "\r\n",
        "    imp_tree_predicts = []\r\n",
        "    tree_predicts = []\r\n",
        "\r\n",
        "    for i in range(0,n_estimators):\r\n",
        "      imp_tr, imp_ts = generate_imp_space(X_train, Y_train, X_test, imp_feature_size, foz)\r\n",
        "      imp_d = imp_tr.shape[1]\r\n",
        "\r\n",
        "      #meta learner params\r\n",
        "      imp_sel_d = 2* round(math.log2(imp_d)) #feature\r\n",
        "      sel_d = 2*round(math.log2(d))\r\n",
        "      \r\n",
        "      imp_tree = RandomForestClassifier(max_features=imp_sel_d, n_estimators=1)#,random_state=42\r\n",
        "      imp_tree.fit(imp_tr, Y_train)\r\n",
        "      imp_tree_predicts.append(imp_tree.predict(imp_ts))\r\n",
        "\r\n",
        "\r\n",
        "      tree = RandomForestClassifier(max_features=sel_d, n_estimators=1)#, random_state=42\r\n",
        "      tree.fit(X_train, Y_train)\r\n",
        "      tree_predicts.append(tree.predict(X_test))\r\n",
        "\r\n",
        "    results_imp = MajorityVoting(np.array(imp_tree_predicts))\r\n",
        "    results = MajorityVoting(np.array(tree_predicts))\r\n",
        "\r\n",
        "    # print(\"--------------  {}  ----------------\".format(ds.upper()))\r\n",
        "    accuracies.append(accuracy_score(Y_test.values, results))\r\n",
        "    accuracies_imp.append(accuracy_score(Y_test.values, results_imp))\r\n",
        "    \r\n",
        "    pt_RF.add_row((ds, \"%.4f\" % accuracy_score(Y_test.values, results_imp),  \"%.4f\" % accuracy_score(Y_test.values, results)))\r\n",
        "  foldPreds_RF[ds] = accuracies\r\n",
        "  foldPreds_RF[ds+\"_imp\"] = accuracies_imp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d4YlaogzCKb",
        "outputId": "5c2621a2-98b0-4229-e5b8-fb452eea8ead"
      },
      "source": [
        "print(pt_RF) "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------+--------+\n",
            "|    Dataset     | Imp_RF |   RF   |\n",
            "+----------------+--------+--------+\n",
            "|   ionosphere   | 0.9318 | 0.9148 |\n",
            "|   ionosphere   | 0.9143 | 0.9086 |\n",
            "|   ionosphere   | 0.9261 | 0.9205 |\n",
            "|   ionosphere   | 0.9486 | 0.8800 |\n",
            "|   ionosphere   | 0.9489 | 0.9318 |\n",
            "|   ionosphere   | 0.9200 | 0.9086 |\n",
            "|   ionosphere   | 0.9432 | 0.9545 |\n",
            "|   ionosphere   | 0.9314 | 0.9200 |\n",
            "|   ionosphere   | 0.9432 | 0.9261 |\n",
            "|   ionosphere   | 0.9543 | 0.9143 |\n",
            "|    diabetes    | 0.7266 | 0.7422 |\n",
            "|    diabetes    | 0.7135 | 0.7005 |\n",
            "|    diabetes    | 0.7630 | 0.7188 |\n",
            "|    diabetes    | 0.7266 | 0.7474 |\n",
            "|    diabetes    | 0.7240 | 0.7135 |\n",
            "|    diabetes    | 0.7161 | 0.6979 |\n",
            "|    diabetes    | 0.7474 | 0.7292 |\n",
            "|    diabetes    | 0.7578 | 0.7552 |\n",
            "|    diabetes    | 0.7734 | 0.7786 |\n",
            "|    diabetes    | 0.7240 | 0.6875 |\n",
            "|     glass      | 0.6990 | 0.7864 |\n",
            "|     glass      | 0.7157 | 0.7157 |\n",
            "|     glass      | 0.5437 | 0.6505 |\n",
            "|     glass      | 0.7059 | 0.6667 |\n",
            "|     glass      | 0.6796 | 0.6893 |\n",
            "|     glass      | 0.7059 | 0.6667 |\n",
            "|     glass      | 0.6796 | 0.7087 |\n",
            "|     glass      | 0.6961 | 0.7549 |\n",
            "|     glass      | 0.6602 | 0.7767 |\n",
            "|     glass      | 0.5784 | 0.6667 |\n",
            "|    abalone     | 0.2186 | 0.2330 |\n",
            "|    abalone     | 0.2327 | 0.2250 |\n",
            "|    abalone     | 0.2460 | 0.2364 |\n",
            "|    abalone     | 0.2389 | 0.2346 |\n",
            "|    abalone     | 0.2388 | 0.2224 |\n",
            "|    abalone     | 0.2298 | 0.2187 |\n",
            "|    abalone     | 0.2330 | 0.2248 |\n",
            "|    abalone     | 0.2447 | 0.2235 |\n",
            "|    abalone     | 0.2383 | 0.2340 |\n",
            "|    abalone     | 0.2216 | 0.2341 |\n",
            "| heart-statlog  | 0.8444 | 0.7630 |\n",
            "| heart-statlog  | 0.8296 | 0.8000 |\n",
            "| heart-statlog  | 0.7704 | 0.7556 |\n",
            "| heart-statlog  | 0.8148 | 0.8000 |\n",
            "| heart-statlog  | 0.7778 | 0.7630 |\n",
            "| heart-statlog  | 0.7926 | 0.8074 |\n",
            "| heart-statlog  | 0.8370 | 0.8296 |\n",
            "| heart-statlog  | 0.8074 | 0.7704 |\n",
            "| heart-statlog  | 0.7852 | 0.8074 |\n",
            "| heart-statlog  | 0.7926 | 0.8296 |\n",
            "|     colic      | 0.8315 | 0.7717 |\n",
            "|     colic      | 0.8261 | 0.8207 |\n",
            "|     colic      | 0.8424 | 0.8043 |\n",
            "|     colic      | 0.8261 | 0.8152 |\n",
            "|     colic      | 0.8261 | 0.8207 |\n",
            "|     colic      | 0.8478 | 0.8098 |\n",
            "|     colic      | 0.8478 | 0.8207 |\n",
            "|     colic      | 0.7826 | 0.7717 |\n",
            "|     colic      | 0.8315 | 0.8043 |\n",
            "|     colic      | 0.8533 | 0.8315 |\n",
            "|    credit-g    | 0.7060 | 0.7200 |\n",
            "|    credit-g    | 0.7280 | 0.7520 |\n",
            "|    credit-g    | 0.7140 | 0.7220 |\n",
            "|    credit-g    | 0.7600 | 0.7380 |\n",
            "|    credit-g    | 0.7080 | 0.6840 |\n",
            "|    credit-g    | 0.7820 | 0.7360 |\n",
            "|    credit-g    | 0.7300 | 0.6960 |\n",
            "|    credit-g    | 0.7240 | 0.7140 |\n",
            "|    credit-g    | 0.7340 | 0.7240 |\n",
            "|    credit-g    | 0.7340 | 0.7540 |\n",
            "| balance-scale  | 0.9361 | 0.7636 |\n",
            "| balance-scale  | 0.9391 | 0.8333 |\n",
            "| balance-scale  | 0.9425 | 0.8275 |\n",
            "| balance-scale  | 0.9135 | 0.8109 |\n",
            "| balance-scale  | 0.9553 | 0.8019 |\n",
            "| balance-scale  | 0.9487 | 0.8141 |\n",
            "| balance-scale  | 0.9201 | 0.8179 |\n",
            "| balance-scale  | 0.8750 | 0.7853 |\n",
            "| balance-scale  | 0.9010 | 0.8147 |\n",
            "| balance-scale  | 0.9231 | 0.8109 |\n",
            "|     autos      | 0.7129 | 0.7426 |\n",
            "|     autos      | 0.6436 | 0.7030 |\n",
            "|     autos      | 0.5842 | 0.6931 |\n",
            "|     autos      | 0.7030 | 0.7228 |\n",
            "|     autos      | 0.6931 | 0.6238 |\n",
            "|     autos      | 0.6040 | 0.5545 |\n",
            "|     autos      | 0.6832 | 0.6634 |\n",
            "|     autos      | 0.6634 | 0.6238 |\n",
            "|     autos      | 0.6733 | 0.7525 |\n",
            "|     autos      | 0.7129 | 0.6832 |\n",
            "|    credit-a    | 0.8580 | 0.8435 |\n",
            "|    credit-a    | 0.8667 | 0.8464 |\n",
            "|    credit-a    | 0.8377 | 0.8493 |\n",
            "|    credit-a    | 0.8696 | 0.8696 |\n",
            "|    credit-a    | 0.8667 | 0.8290 |\n",
            "|    credit-a    | 0.8522 | 0.8638 |\n",
            "|    credit-a    | 0.8435 | 0.8493 |\n",
            "|    credit-a    | 0.8522 | 0.8725 |\n",
            "|    credit-a    | 0.8638 | 0.8493 |\n",
            "|    credit-a    | 0.8667 | 0.8609 |\n",
            "|    breast-w    | 0.9600 | 0.9543 |\n",
            "|    breast-w    | 0.9513 | 0.9456 |\n",
            "|    breast-w    | 0.9571 | 0.9429 |\n",
            "|    breast-w    | 0.9713 | 0.9570 |\n",
            "|    breast-w    | 0.9543 | 0.9514 |\n",
            "|    breast-w    | 0.9713 | 0.9513 |\n",
            "|    breast-w    | 0.9686 | 0.9571 |\n",
            "|    breast-w    | 0.9542 | 0.9542 |\n",
            "|    breast-w    | 0.9571 | 0.9514 |\n",
            "|    breast-w    | 0.9656 | 0.9656 |\n",
            "| breast-cancer  | 0.6853 | 0.7063 |\n",
            "| breast-cancer  | 0.7063 | 0.6993 |\n",
            "| breast-cancer  | 0.7483 | 0.7413 |\n",
            "| breast-cancer  | 0.7133 | 0.7133 |\n",
            "| breast-cancer  | 0.7063 | 0.6993 |\n",
            "| breast-cancer  | 0.7133 | 0.6853 |\n",
            "| breast-cancer  | 0.7273 | 0.7832 |\n",
            "| breast-cancer  | 0.7063 | 0.7133 |\n",
            "| breast-cancer  | 0.6923 | 0.7133 |\n",
            "| breast-cancer  | 0.7203 | 0.7203 |\n",
            "|   audiology    | 0.9176 | 0.8235 |\n",
            "|   audiology    | 0.8571 | 0.8571 |\n",
            "|   audiology    | 0.8941 | 0.7882 |\n",
            "|   audiology    | 0.8810 | 0.9048 |\n",
            "|   audiology    | 0.8706 | 0.8706 |\n",
            "|   audiology    | 0.8095 | 0.8333 |\n",
            "|   audiology    | 0.8588 | 0.7882 |\n",
            "|   audiology    | 0.8929 | 0.9048 |\n",
            "|   audiology    | 0.8235 | 0.7412 |\n",
            "|   audiology    | 0.8810 | 0.8333 |\n",
            "|     lymph      | 0.7324 | 0.7042 |\n",
            "|     lymph      | 0.9155 | 0.9014 |\n",
            "|     lymph      | 0.8310 | 0.7746 |\n",
            "|     lymph      | 0.8169 | 0.8028 |\n",
            "|     lymph      | 0.8310 | 0.8592 |\n",
            "|     lymph      | 0.7887 | 0.7183 |\n",
            "|     lymph      | 0.8028 | 0.8873 |\n",
            "|     lymph      | 0.7887 | 0.8169 |\n",
            "|     lymph      | 0.7746 | 0.8310 |\n",
            "|     lymph      | 0.8451 | 0.8732 |\n",
            "|   hepatitis    | 0.7949 | 0.8077 |\n",
            "|   hepatitis    | 0.8571 | 0.8182 |\n",
            "|   hepatitis    | 0.8974 | 0.8333 |\n",
            "|   hepatitis    | 0.8312 | 0.8442 |\n",
            "|   hepatitis    | 0.7821 | 0.8333 |\n",
            "|   hepatitis    | 0.8312 | 0.8442 |\n",
            "|   hepatitis    | 0.8462 | 0.8718 |\n",
            "|   hepatitis    | 0.8701 | 0.8442 |\n",
            "|   hepatitis    | 0.8462 | 0.8205 |\n",
            "|   hepatitis    | 0.8571 | 0.7922 |\n",
            "|     labor      | 0.8621 | 0.8966 |\n",
            "|     labor      | 0.7500 | 0.8571 |\n",
            "|     labor      | 0.7931 | 0.8276 |\n",
            "|     labor      | 0.8929 | 0.8571 |\n",
            "|     labor      | 0.9310 | 0.8621 |\n",
            "|     labor      | 0.8929 | 1.0000 |\n",
            "|     labor      | 0.8966 | 0.8621 |\n",
            "|     labor      | 0.8929 | 0.9643 |\n",
            "|     labor      | 0.8966 | 0.9310 |\n",
            "|     labor      | 0.8214 | 0.8929 |\n",
            "|      iris      | 0.9733 | 0.9467 |\n",
            "|      iris      | 0.9867 | 0.9600 |\n",
            "|      iris      | 0.9467 | 0.9467 |\n",
            "|      iris      | 0.9467 | 0.9333 |\n",
            "|      iris      | 0.9733 | 0.9733 |\n",
            "|      iris      | 0.9333 | 0.9333 |\n",
            "|      iris      | 0.9600 | 0.9333 |\n",
            "|      iris      | 0.9733 | 0.9867 |\n",
            "|      iris      | 0.9467 | 0.9600 |\n",
            "|      iris      | 0.9867 | 0.9467 |\n",
            "|     letter     | 0.9126 | 0.9159 |\n",
            "|     letter     | 0.9093 | 0.9165 |\n",
            "|     letter     | 0.9127 | 0.9178 |\n",
            "|     letter     | 0.9044 | 0.9192 |\n",
            "|     letter     | 0.9154 | 0.9163 |\n",
            "|     letter     | 0.9135 | 0.9122 |\n",
            "|     letter     | 0.9059 | 0.9126 |\n",
            "|     letter     | 0.9124 | 0.9184 |\n",
            "|     letter     | 0.9102 | 0.9189 |\n",
            "|     letter     | 0.9090 | 0.9158 |\n",
            "|     vowel      | 0.8465 | 0.8384 |\n",
            "|     vowel      | 0.8485 | 0.8141 |\n",
            "|     vowel      | 0.8384 | 0.8586 |\n",
            "|     vowel      | 0.8323 | 0.8202 |\n",
            "|     vowel      | 0.8828 | 0.8566 |\n",
            "|     vowel      | 0.8303 | 0.7838 |\n",
            "|     vowel      | 0.8788 | 0.8141 |\n",
            "|     vowel      | 0.8525 | 0.8263 |\n",
            "|     vowel      | 0.8384 | 0.8323 |\n",
            "|     vowel      | 0.8485 | 0.8162 |\n",
            "|    soybean     | 0.9290 | 0.9260 |\n",
            "|    soybean     | 0.8754 | 0.8902 |\n",
            "|    soybean     | 0.9083 | 0.9053 |\n",
            "|    soybean     | 0.9050 | 0.9139 |\n",
            "|    soybean     | 0.9290 | 0.9260 |\n",
            "|    soybean     | 0.8783 | 0.8902 |\n",
            "|    soybean     | 0.9290 | 0.9112 |\n",
            "|    soybean     | 0.9169 | 0.9199 |\n",
            "|    soybean     | 0.9053 | 0.8876 |\n",
            "|    soybean     | 0.9318 | 0.9110 |\n",
            "|    segment     | 0.9706 | 0.9593 |\n",
            "|    segment     | 0.9610 | 0.9671 |\n",
            "|    segment     | 0.9636 | 0.9645 |\n",
            "|    segment     | 0.9593 | 0.9645 |\n",
            "|    segment     | 0.9697 | 0.9662 |\n",
            "|    segment     | 0.9766 | 0.9671 |\n",
            "|    segment     | 0.9671 | 0.9506 |\n",
            "|    segment     | 0.9697 | 0.9723 |\n",
            "|    segment     | 0.9706 | 0.9662 |\n",
            "|    segment     | 0.9654 | 0.9532 |\n",
            "|      vote      | 0.9633 | 0.9404 |\n",
            "|      vote      | 0.9677 | 0.9631 |\n",
            "|      vote      | 0.9679 | 0.9725 |\n",
            "|      vote      | 0.9493 | 0.9493 |\n",
            "|      vote      | 0.9266 | 0.9220 |\n",
            "|      vote      | 0.9631 | 0.9677 |\n",
            "|      vote      | 0.9587 | 0.9679 |\n",
            "|      vote      | 0.9631 | 0.9585 |\n",
            "|      vote      | 0.9541 | 0.9450 |\n",
            "|      vote      | 0.9631 | 0.9724 |\n",
            "|     sonar      | 0.7692 | 0.7404 |\n",
            "|     sonar      | 0.7596 | 0.7115 |\n",
            "|     sonar      | 0.7596 | 0.7596 |\n",
            "|     sonar      | 0.7981 | 0.7692 |\n",
            "|     sonar      | 0.8750 | 0.7596 |\n",
            "|     sonar      | 0.7500 | 0.6635 |\n",
            "|     sonar      | 0.7019 | 0.7212 |\n",
            "|     sonar      | 0.7115 | 0.7019 |\n",
            "|     sonar      | 0.8558 | 0.8173 |\n",
            "|     sonar      | 0.7885 | 0.7115 |\n",
            "|      zoo       | 1.0000 | 0.9762 |\n",
            "|      zoo       | 0.9524 | 1.0000 |\n",
            "|      zoo       | 0.9762 | 0.9762 |\n",
            "|      zoo       | 1.0000 | 1.0000 |\n",
            "|      zoo       | 1.0000 | 1.0000 |\n",
            "|      zoo       | 0.9762 | 1.0000 |\n",
            "|      zoo       | 0.9762 | 0.9762 |\n",
            "|      zoo       | 1.0000 | 1.0000 |\n",
            "|      zoo       | 1.0000 | 1.0000 |\n",
            "|      zoo       | 0.9762 | 1.0000 |\n",
            "|    vehicle     | 0.7470 | 0.7329 |\n",
            "|    vehicle     | 0.7447 | 0.7045 |\n",
            "|    vehicle     | 0.7400 | 0.7021 |\n",
            "|    vehicle     | 0.7589 | 0.7281 |\n",
            "|    vehicle     | 0.8061 | 0.7187 |\n",
            "|    vehicle     | 0.7376 | 0.7187 |\n",
            "|    vehicle     | 0.7423 | 0.6903 |\n",
            "|    vehicle     | 0.7518 | 0.7376 |\n",
            "|    vehicle     | 0.7400 | 0.7116 |\n",
            "|    vehicle     | 0.7187 | 0.6714 |\n",
            "|    waveform    | 0.8332 | 0.8116 |\n",
            "|    waveform    | 0.8288 | 0.8100 |\n",
            "|    waveform    | 0.8212 | 0.8132 |\n",
            "|    waveform    | 0.8232 | 0.8172 |\n",
            "|    waveform    | 0.8236 | 0.8168 |\n",
            "|    waveform    | 0.8320 | 0.8208 |\n",
            "|    waveform    | 0.8284 | 0.8200 |\n",
            "|    waveform    | 0.8360 | 0.8092 |\n",
            "|    waveform    | 0.8412 | 0.8168 |\n",
            "|    waveform    | 0.8288 | 0.8124 |\n",
            "| primary-tumor  | 0.4834 | 0.4371 |\n",
            "| primary-tumor  | 0.4371 | 0.4040 |\n",
            "| primary-tumor  | 0.4570 | 0.4371 |\n",
            "| primary-tumor  | 0.4503 | 0.4040 |\n",
            "| primary-tumor  | 0.4106 | 0.4172 |\n",
            "| primary-tumor  | 0.4172 | 0.4371 |\n",
            "| primary-tumor  | 0.4172 | 0.4172 |\n",
            "| primary-tumor  | 0.4238 | 0.4106 |\n",
            "| primary-tumor  | 0.5232 | 0.4967 |\n",
            "| primary-tumor  | 0.4437 | 0.4238 |\n",
            "|     splice     | 0.9386 | 0.9260 |\n",
            "|     splice     | 0.9273 | 0.9091 |\n",
            "|     splice     | 0.9429 | 0.9266 |\n",
            "|     splice     | 0.9310 | 0.9179 |\n",
            "|     splice     | 0.9260 | 0.9166 |\n",
            "|     splice     | 0.9367 | 0.9266 |\n",
            "|     splice     | 0.9379 | 0.8884 |\n",
            "|     splice     | 0.9348 | 0.9235 |\n",
            "|     splice     | 0.9279 | 0.9292 |\n",
            "|     splice     | 0.9361 | 0.9072 |\n",
            "|    ringnorm    | 0.9757 | 0.9265 |\n",
            "|    ringnorm    | 0.9746 | 0.9308 |\n",
            "|    ringnorm    | 0.9751 | 0.9284 |\n",
            "|    ringnorm    | 0.9808 | 0.9419 |\n",
            "|    ringnorm    | 0.9803 | 0.9297 |\n",
            "|    ringnorm    | 0.9781 | 0.9368 |\n",
            "|    ringnorm    | 0.9786 | 0.9438 |\n",
            "|    ringnorm    | 0.9786 | 0.9335 |\n",
            "|    ringnorm    | 0.9795 | 0.9405 |\n",
            "|    ringnorm    | 0.9811 | 0.9281 |\n",
            "|     col10      | 0.7802 | 0.7851 |\n",
            "|     col10      | 0.7750 | 0.7750 |\n",
            "|     col10      | 0.7832 | 0.7950 |\n",
            "|     col10      | 0.7631 | 0.7661 |\n",
            "|     col10      | 0.7584 | 0.7574 |\n",
            "|     col10      | 0.7701 | 0.7721 |\n",
            "|     col10      | 0.7802 | 0.7812 |\n",
            "|     col10      | 0.7721 | 0.7958 |\n",
            "|     col10      | 0.7822 | 0.8079 |\n",
            "|     col10      | 0.7849 | 0.7849 |\n",
            "|      cmc       | 0.5075 | 0.5210 |\n",
            "|      cmc       | 0.4986 | 0.5367 |\n",
            "|      cmc       | 0.5020 | 0.5034 |\n",
            "|      cmc       | 0.5340 | 0.5258 |\n",
            "|      cmc       | 0.5387 | 0.5156 |\n",
            "|      cmc       | 0.5082 | 0.5122 |\n",
            "|      cmc       | 0.5414 | 0.4993 |\n",
            "|      cmc       | 0.5149 | 0.4946 |\n",
            "|      cmc       | 0.5360 | 0.5034 |\n",
            "|      cmc       | 0.5312 | 0.5408 |\n",
            "|    column3C    | 0.8387 | 0.8258 |\n",
            "|    column3C    | 0.8452 | 0.8516 |\n",
            "|    column3C    | 0.8516 | 0.8581 |\n",
            "|    column3C    | 0.8194 | 0.8258 |\n",
            "|    column3C    | 0.8968 | 0.8710 |\n",
            "|    column3C    | 0.8323 | 0.8194 |\n",
            "|    column3C    | 0.8516 | 0.8065 |\n",
            "|    column3C    | 0.8387 | 0.7871 |\n",
            "|    column3C    | 0.8581 | 0.8194 |\n",
            "|    column3C    | 0.8452 | 0.8065 |\n",
            "|     ecoli      | 0.8110 | 0.8110 |\n",
            "|     ecoli      | 0.8773 | 0.8221 |\n",
            "|     ecoli      | 0.8963 | 0.8537 |\n",
            "|     ecoli      | 0.8712 | 0.8221 |\n",
            "|     ecoli      | 0.8537 | 0.8110 |\n",
            "|     ecoli      | 0.8528 | 0.8037 |\n",
            "|     ecoli      | 0.8232 | 0.8598 |\n",
            "|     ecoli      | 0.9018 | 0.8589 |\n",
            "|     ecoli      | 0.8049 | 0.7561 |\n",
            "|     ecoli      | 0.9018 | 0.8712 |\n",
            "|      monk      | 0.9344 | 0.9344 |\n",
            "|      monk      | 0.8361 | 0.9180 |\n",
            "|      monk      | 0.8689 | 0.9180 |\n",
            "|      monk      | 0.8852 | 0.8852 |\n",
            "|      monk      | 0.9180 | 0.9180 |\n",
            "|      monk      | 0.8852 | 0.9180 |\n",
            "|      monk      | 0.9344 | 0.9016 |\n",
            "|      monk      | 0.8689 | 0.8689 |\n",
            "|      monk      | 0.9508 | 0.9180 |\n",
            "|      monk      | 0.9180 | 0.9180 |\n",
            "|  transfusion   | 0.7380 | 0.7193 |\n",
            "|  transfusion   | 0.7406 | 0.7193 |\n",
            "|  transfusion   | 0.7406 | 0.7513 |\n",
            "|  transfusion   | 0.7273 | 0.7219 |\n",
            "|  transfusion   | 0.7540 | 0.7299 |\n",
            "|  transfusion   | 0.7299 | 0.7299 |\n",
            "|  transfusion   | 0.7567 | 0.7380 |\n",
            "|  transfusion   | 0.7326 | 0.7059 |\n",
            "|  transfusion   | 0.7059 | 0.7326 |\n",
            "|  transfusion   | 0.7326 | 0.7540 |\n",
            "| wineCultivars  | 0.9870 | 0.9740 |\n",
            "| wineCultivars  | 0.9474 | 0.9737 |\n",
            "| wineCultivars  | 0.9610 | 0.9610 |\n",
            "| wineCultivars  | 0.9737 | 0.9605 |\n",
            "| wineCultivars  | 0.9740 | 0.9610 |\n",
            "| wineCultivars  | 0.8947 | 0.9211 |\n",
            "| wineCultivars  | 0.9610 | 0.9351 |\n",
            "| wineCultivars  | 0.9737 | 0.9342 |\n",
            "| wineCultivars  | 0.9351 | 0.9481 |\n",
            "| wineCultivars  | 0.9868 | 1.0000 |\n",
            "|   hillValley   | 0.9835 | 0.4950 |\n",
            "|   hillValley   | 0.9538 | 0.5083 |\n",
            "|   hillValley   | 0.9406 | 0.5314 |\n",
            "|   hillValley   | 0.9901 | 0.5611 |\n",
            "|   hillValley   | 0.9637 | 0.5248 |\n",
            "|   hillValley   | 0.9736 | 0.4917 |\n",
            "|   hillValley   | 0.9736 | 0.5842 |\n",
            "|   hillValley   | 0.9670 | 0.5743 |\n",
            "|   hillValley   | 0.9637 | 0.5809 |\n",
            "|   hillValley   | 0.9769 | 0.5413 |\n",
            "| movementLibras | 0.6833 | 0.6444 |\n",
            "| movementLibras | 0.7056 | 0.5889 |\n",
            "| movementLibras | 0.6556 | 0.6556 |\n",
            "| movementLibras | 0.7000 | 0.6500 |\n",
            "| movementLibras | 0.7056 | 0.6722 |\n",
            "| movementLibras | 0.7000 | 0.6444 |\n",
            "| movementLibras | 0.7056 | 0.6111 |\n",
            "| movementLibras | 0.6944 | 0.6222 |\n",
            "| movementLibras | 0.7111 | 0.6500 |\n",
            "| movementLibras | 0.7278 | 0.6778 |\n",
            "|    spambase    | 0.9318 | 0.9200 |\n",
            "|    spambase    | 0.9187 | 0.9222 |\n",
            "|    spambase    | 0.9248 | 0.9248 |\n",
            "|    spambase    | 0.9235 | 0.9291 |\n",
            "|    spambase    | 0.9205 | 0.9318 |\n",
            "|    spambase    | 0.9196 | 0.9200 |\n",
            "|    spambase    | 0.9213 | 0.9235 |\n",
            "|    spambase    | 0.9313 | 0.9196 |\n",
            "|    spambase    | 0.9283 | 0.9187 |\n",
            "|    spambase    | 0.9222 | 0.9170 |\n",
            "|     yeast      | 0.5608 | 0.5608 |\n",
            "|     yeast      | 0.5737 | 0.5616 |\n",
            "|     yeast      | 0.5581 | 0.5824 |\n",
            "|     yeast      | 0.5656 | 0.5683 |\n",
            "|     yeast      | 0.6014 | 0.5757 |\n",
            "|     yeast      | 0.5697 | 0.5345 |\n",
            "|     yeast      | 0.5811 | 0.5365 |\n",
            "|     yeast      | 0.5724 | 0.5900 |\n",
            "|     yeast      | 0.5595 | 0.5392 |\n",
            "|     yeast      | 0.5724 | 0.5575 |\n",
            "|  iphonetweets  | 0.7556 | 0.7669 |\n",
            "|  iphonetweets  | 0.7669 | 0.7782 |\n",
            "|  iphonetweets  | 0.7820 | 0.7970 |\n",
            "|  iphonetweets  | 0.7256 | 0.7180 |\n",
            "|  iphonetweets  | 0.7669 | 0.7669 |\n",
            "|  iphonetweets  | 0.7406 | 0.7519 |\n",
            "|  iphonetweets  | 0.7331 | 0.7368 |\n",
            "|  iphonetweets  | 0.7632 | 0.7932 |\n",
            "|  iphonetweets  | 0.7030 | 0.7180 |\n",
            "|  iphonetweets  | 0.7782 | 0.7744 |\n",
            "|  hobbittweets  | 0.9349 | 0.9195 |\n",
            "|  hobbittweets  | 0.9272 | 0.9234 |\n",
            "|  hobbittweets  | 0.9119 | 0.9042 |\n",
            "|  hobbittweets  | 0.9272 | 0.9234 |\n",
            "|  hobbittweets  | 0.9425 | 0.9310 |\n",
            "|  hobbittweets  | 0.9540 | 0.9464 |\n",
            "|  hobbittweets  | 0.9042 | 0.9042 |\n",
            "|  hobbittweets  | 0.9425 | 0.9195 |\n",
            "|  hobbittweets  | 0.9540 | 0.9425 |\n",
            "|  hobbittweets  | 0.8889 | 0.9042 |\n",
            "+----------------+--------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5md9xSXlqbcV"
      },
      "source": [
        "np.save(ds_folderpath + \"/\" + 'RF.npy',foldPreds_RF ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxskWdO9mg5U"
      },
      "source": [
        "Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK4ZUtX7vCE5"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\r\n",
        "from sklearn.model_selection import RepeatedKFold\r\n",
        "foz=4\r\n",
        "imp_feature_size=1\r\n",
        "n_estimators = 10\r\n",
        "#Table\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "pt_Bagging = PrettyTable()\r\n",
        "\r\n",
        "pt_Bagging.field_names = [\"Dataset\", \"Imp_Bagging\", \"Bagging\"]\r\n",
        "#\r\n",
        "foldPreds_Bagging = {}\r\n",
        "K =0\r\n",
        "for ds in datasets:\r\n",
        "  K+=1\r\n",
        "  rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=2652124)\r\n",
        "  if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "    df = datasets[ds]\r\n",
        "    X = df['text']\r\n",
        "    Y = df['label']\r\n",
        "  else:\r\n",
        "    X,Y = datasets[ds]\r\n",
        "    Y = Y.str.decode(\"utf-8\")\r\n",
        "    \r\n",
        "  # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\r\n",
        "  print(ds)\r\n",
        "  accuracies = []\r\n",
        "  accuracies_imp = []  \r\n",
        "  for train_index, test_index in rkf.split(X):\r\n",
        "\r\n",
        "    X_train = X.iloc[train_index]\r\n",
        "    X_test = X.iloc[test_index]\r\n",
        "    Y_train = Y.iloc[train_index]\r\n",
        "    Y_test = Y.iloc[test_index]\r\n",
        "\r\n",
        "    if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "      vectorizer = CountVectorizer(analyzer=preprocess)\r\n",
        "      vectorizer.fit(X_train)\r\n",
        "\r\n",
        "      X_train = vectorizer.transform(X_train).toarray()\r\n",
        "      X_test = vectorizer.transform(X_test).toarray()\r\n",
        "      X_train = pd.DataFrame(X_train)\r\n",
        "      X_test = pd.DataFrame(X_test)\r\n",
        "\r\n",
        "    d = len(X_train.columns)\r\n",
        "    num_class = len(Y_train.value_counts())\r\n",
        "\r\n",
        "\r\n",
        "    imp_tree_predicts = []\r\n",
        "    tree_predicts = []\r\n",
        "\r\n",
        "    for i in range(0,n_estimators):\r\n",
        "      imp_tr, imp_ts = generate_imp_space(X_train, Y_train, X_test, imp_feature_size, foz)\r\n",
        "      imp_d = imp_tr.shape[1]   \r\n",
        "      \r\n",
        "      imp_tree = BaggingClassifier(n_estimators=1)#,random_state=42\r\n",
        "      imp_tree.fit(imp_tr, Y_train)\r\n",
        "      imp_tree_predicts.append(imp_tree.predict(imp_ts))\r\n",
        "\r\n",
        "\r\n",
        "      tree = BaggingClassifier(n_estimators=1)#, random_state=42\r\n",
        "      tree.fit(X_train, Y_train)\r\n",
        "      tree_predicts.append(tree.predict(X_test))\r\n",
        "\r\n",
        "    results_imp = MajorityVoting(np.array(imp_tree_predicts))\r\n",
        "    results = MajorityVoting(np.array(tree_predicts))\r\n",
        "\r\n",
        "    # print(\"--------------  {}  ----------------\".format(ds.upper()))\r\n",
        "    accuracies.append(accuracy_score(Y_test.values, results))\r\n",
        "    accuracies_imp.append(accuracy_score(Y_test.values, results_imp))\r\n",
        "    # print(\" \")\r\n",
        "    pt_Bagging.add_row((ds, \"%.4f\" % accuracy_score(Y_test.values, results_imp),  \"%.4f\" % accuracy_score(Y_test.values, results)))\r\n",
        "\r\n",
        "  foldPreds_Bagging[ds] = accuracies\r\n",
        "  foldPreds_Bagging[ds+\"_imp\"] = accuracies_imp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeGnS84dagg7",
        "outputId": "6c63fa40-8e41-4b7f-81c1-22d0bc37b1c2"
      },
      "source": [
        "print(pt_Bagging) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+-------------+---------+\n",
            "|    Dataset     | Imp_Bagging | Bagging |\n",
            "+----------------+-------------+---------+\n",
            "|   ionosphere   |    0.9261   |  0.8977 |\n",
            "|   ionosphere   |    0.9143   |  0.9143 |\n",
            "|   ionosphere   |    0.9375   |  0.9034 |\n",
            "|   ionosphere   |    0.9543   |  0.8800 |\n",
            "|   ionosphere   |    0.9375   |  0.9318 |\n",
            "|   ionosphere   |    0.9086   |  0.8971 |\n",
            "|   ionosphere   |    0.9545   |  0.9716 |\n",
            "|   ionosphere   |    0.9143   |  0.9029 |\n",
            "|   ionosphere   |    0.9432   |  0.8920 |\n",
            "|   ionosphere   |    0.9200   |  0.9029 |\n",
            "|    diabetes    |    0.7292   |  0.7422 |\n",
            "|    diabetes    |    0.7188   |  0.7240 |\n",
            "|    diabetes    |    0.7448   |  0.7448 |\n",
            "|    diabetes    |    0.7240   |  0.7240 |\n",
            "|    diabetes    |    0.7318   |  0.7526 |\n",
            "|    diabetes    |    0.7344   |  0.7135 |\n",
            "|    diabetes    |    0.7266   |  0.7292 |\n",
            "|    diabetes    |    0.7448   |  0.7396 |\n",
            "|    diabetes    |    0.7552   |  0.7604 |\n",
            "|    diabetes    |    0.7057   |  0.6901 |\n",
            "|     glass      |    0.7184   |  0.7184 |\n",
            "|     glass      |    0.6961   |  0.6569 |\n",
            "|     glass      |    0.6117   |  0.6699 |\n",
            "|     glass      |    0.6961   |  0.6863 |\n",
            "|     glass      |    0.6408   |  0.6990 |\n",
            "|     glass      |    0.7255   |  0.6373 |\n",
            "|     glass      |    0.7087   |  0.7184 |\n",
            "|     glass      |    0.6471   |  0.7451 |\n",
            "|     glass      |    0.6796   |  0.7767 |\n",
            "|     glass      |    0.5784   |  0.6373 |\n",
            "|    abalone     |    0.2297   |  0.2186 |\n",
            "|    abalone     |    0.2172   |  0.2365 |\n",
            "|    abalone     |    0.2200   |  0.2321 |\n",
            "|    abalone     |    0.2457   |  0.2216 |\n",
            "|    abalone     |    0.2195   |  0.2325 |\n",
            "|    abalone     |    0.2447   |  0.2442 |\n",
            "|    abalone     |    0.2345   |  0.2335 |\n",
            "|    abalone     |    0.2399   |  0.2452 |\n",
            "|    abalone     |    0.2403   |  0.2200 |\n",
            "|    abalone     |    0.2322   |  0.2259 |\n",
            "| heart-statlog  |    0.8148   |  0.7704 |\n",
            "| heart-statlog  |    0.8519   |  0.7926 |\n",
            "| heart-statlog  |    0.7852   |  0.7630 |\n",
            "| heart-statlog  |    0.8148   |  0.7926 |\n",
            "| heart-statlog  |    0.7704   |  0.7704 |\n",
            "| heart-statlog  |    0.8222   |  0.8074 |\n",
            "| heart-statlog  |    0.8370   |  0.7704 |\n",
            "| heart-statlog  |    0.7852   |  0.7852 |\n",
            "| heart-statlog  |    0.8370   |  0.7333 |\n",
            "| heart-statlog  |    0.7778   |  0.7926 |\n",
            "|     colic      |    0.8098   |  0.8261 |\n",
            "|     colic      |    0.8207   |  0.8261 |\n",
            "|     colic      |    0.8478   |  0.8370 |\n",
            "|     colic      |    0.7935   |  0.8207 |\n",
            "|     colic      |    0.8261   |  0.8424 |\n",
            "|     colic      |    0.8261   |  0.8478 |\n",
            "|     colic      |    0.8098   |  0.8315 |\n",
            "|     colic      |    0.8261   |  0.8587 |\n",
            "|     colic      |    0.8261   |  0.8261 |\n",
            "|     colic      |    0.8424   |  0.8043 |\n",
            "|    credit-g    |    0.7260   |  0.7280 |\n",
            "|    credit-g    |    0.7180   |  0.7360 |\n",
            "|    credit-g    |    0.7320   |  0.7180 |\n",
            "|    credit-g    |    0.7560   |  0.7540 |\n",
            "|    credit-g    |    0.7320   |  0.7360 |\n",
            "|    credit-g    |    0.7660   |  0.7420 |\n",
            "|    credit-g    |    0.7120   |  0.7160 |\n",
            "|    credit-g    |    0.7240   |  0.6960 |\n",
            "|    credit-g    |    0.7400   |  0.7260 |\n",
            "|    credit-g    |    0.7480   |  0.7120 |\n",
            "| balance-scale  |    0.9361   |  0.7923 |\n",
            "| balance-scale  |    0.9391   |  0.8462 |\n",
            "| balance-scale  |    0.9521   |  0.8371 |\n",
            "| balance-scale  |    0.9135   |  0.7949 |\n",
            "| balance-scale  |    0.9329   |  0.8083 |\n",
            "| balance-scale  |    0.9423   |  0.7788 |\n",
            "| balance-scale  |    0.9265   |  0.8211 |\n",
            "| balance-scale  |    0.8718   |  0.7596 |\n",
            "| balance-scale  |    0.9201   |  0.8083 |\n",
            "| balance-scale  |    0.9551   |  0.8205 |\n",
            "|     autos      |    0.7723   |  0.7228 |\n",
            "|     autos      |    0.7525   |  0.7228 |\n",
            "|     autos      |    0.5842   |  0.6436 |\n",
            "|     autos      |    0.6733   |  0.7426 |\n",
            "|     autos      |    0.6535   |  0.6733 |\n",
            "|     autos      |    0.5545   |  0.6139 |\n",
            "|     autos      |    0.7822   |  0.7228 |\n",
            "|     autos      |    0.6436   |  0.6337 |\n",
            "|     autos      |    0.7327   |  0.7228 |\n",
            "|     autos      |    0.7129   |  0.7228 |\n",
            "|    credit-a    |    0.8348   |  0.8290 |\n",
            "|    credit-a    |    0.8580   |  0.8580 |\n",
            "|    credit-a    |    0.8522   |  0.8261 |\n",
            "|    credit-a    |    0.8696   |  0.8406 |\n",
            "|    credit-a    |    0.8406   |  0.8522 |\n",
            "|    credit-a    |    0.8493   |  0.8638 |\n",
            "|    credit-a    |    0.8493   |  0.8551 |\n",
            "|    credit-a    |    0.8377   |  0.8290 |\n",
            "|    credit-a    |    0.8464   |  0.8232 |\n",
            "|    credit-a    |    0.8638   |  0.8493 |\n",
            "|    breast-w    |    0.9657   |  0.9543 |\n",
            "|    breast-w    |    0.9628   |  0.9284 |\n",
            "|    breast-w    |    0.9571   |  0.9457 |\n",
            "|    breast-w    |    0.9742   |  0.9513 |\n",
            "|    breast-w    |    0.9600   |  0.9486 |\n",
            "|    breast-w    |    0.9656   |  0.9542 |\n",
            "|    breast-w    |    0.9743   |  0.9486 |\n",
            "|    breast-w    |    0.9628   |  0.9513 |\n",
            "|    breast-w    |    0.9457   |  0.9514 |\n",
            "|    breast-w    |    0.9685   |  0.9713 |\n",
            "| breast-cancer  |    0.7413   |  0.7133 |\n",
            "| breast-cancer  |    0.7343   |  0.6294 |\n",
            "| breast-cancer  |    0.6993   |  0.6853 |\n",
            "| breast-cancer  |    0.7133   |  0.7063 |\n",
            "| breast-cancer  |    0.7133   |  0.7133 |\n",
            "| breast-cancer  |    0.7133   |  0.6853 |\n",
            "| breast-cancer  |    0.7273   |  0.6573 |\n",
            "| breast-cancer  |    0.6643   |  0.6643 |\n",
            "| breast-cancer  |    0.7203   |  0.6643 |\n",
            "| breast-cancer  |    0.7063   |  0.7133 |\n",
            "|   audiology    |    0.8941   |  0.8588 |\n",
            "|   audiology    |    0.8095   |  0.8095 |\n",
            "|   audiology    |    0.8471   |  0.8706 |\n",
            "|   audiology    |    0.8929   |  0.8929 |\n",
            "|   audiology    |    0.8706   |  0.8706 |\n",
            "|   audiology    |    0.9167   |  0.8571 |\n",
            "|   audiology    |    0.8824   |  0.8588 |\n",
            "|   audiology    |    0.8452   |  0.8810 |\n",
            "|   audiology    |    0.7765   |  0.8118 |\n",
            "|   audiology    |    0.8810   |  0.8810 |\n",
            "|     lymph      |    0.7746   |  0.7042 |\n",
            "|     lymph      |    0.8873   |  0.8873 |\n",
            "|     lymph      |    0.7746   |  0.8169 |\n",
            "|     lymph      |    0.8873   |  0.8310 |\n",
            "|     lymph      |    0.8732   |  0.8873 |\n",
            "|     lymph      |    0.7746   |  0.7887 |\n",
            "|     lymph      |    0.7887   |  0.8732 |\n",
            "|     lymph      |    0.8169   |  0.7887 |\n",
            "|     lymph      |    0.7746   |  0.7324 |\n",
            "|     lymph      |    0.8451   |  0.8592 |\n",
            "|   hepatitis    |    0.7821   |  0.8077 |\n",
            "|   hepatitis    |    0.7922   |  0.8312 |\n",
            "|   hepatitis    |    0.8718   |  0.8077 |\n",
            "|   hepatitis    |    0.8442   |  0.8182 |\n",
            "|   hepatitis    |    0.8462   |  0.8077 |\n",
            "|   hepatitis    |    0.8961   |  0.7922 |\n",
            "|   hepatitis    |    0.8333   |  0.8462 |\n",
            "|   hepatitis    |    0.8442   |  0.8182 |\n",
            "|   hepatitis    |    0.8205   |  0.8205 |\n",
            "|   hepatitis    |    0.7922   |  0.8052 |\n",
            "|     labor      |    0.8621   |  0.8621 |\n",
            "|     labor      |    0.7857   |  0.7500 |\n",
            "|     labor      |    0.7931   |  0.8621 |\n",
            "|     labor      |    0.8929   |  0.7857 |\n",
            "|     labor      |    0.9310   |  0.7931 |\n",
            "|     labor      |    0.9286   |  0.8571 |\n",
            "|     labor      |    0.9310   |  0.9655 |\n",
            "|     labor      |    0.9643   |  1.0000 |\n",
            "|     labor      |    0.9655   |  0.8966 |\n",
            "|     labor      |    0.7857   |  0.8929 |\n",
            "|      iris      |    0.9733   |  0.9333 |\n",
            "|      iris      |    0.9867   |  0.9600 |\n",
            "|      iris      |    0.9467   |  0.9467 |\n",
            "|      iris      |    0.9733   |  0.9467 |\n",
            "|      iris      |    0.9733   |  0.9867 |\n",
            "|      iris      |    0.9333   |  0.9467 |\n",
            "|      iris      |    0.9467   |  0.9333 |\n",
            "|      iris      |    0.9867   |  0.9200 |\n",
            "|      iris      |    0.9467   |  0.9600 |\n",
            "|      iris      |    0.9600   |  0.9467 |\n",
            "|     letter     |    0.9117   |  0.8990 |\n",
            "|     letter     |    0.9126   |  0.9110 |\n",
            "|     letter     |    0.9138   |  0.9123 |\n",
            "|     letter     |    0.9110   |  0.9089 |\n",
            "|     letter     |    0.9092   |  0.9096 |\n",
            "|     letter     |    0.9131   |  0.9038 |\n",
            "|     letter     |    0.9080   |  0.9023 |\n",
            "|     letter     |    0.9132   |  0.9028 |\n",
            "|     letter     |    0.9076   |  0.9092 |\n",
            "|     letter     |    0.9137   |  0.9040 |\n",
            "|     vowel      |    0.8646   |  0.8384 |\n",
            "|     vowel      |    0.8384   |  0.7939 |\n",
            "|     vowel      |    0.8384   |  0.7939 |\n",
            "|     vowel      |    0.8384   |  0.8182 |\n",
            "|     vowel      |    0.8788   |  0.8909 |\n",
            "|     vowel      |    0.8364   |  0.7798 |\n",
            "|     vowel      |    0.8566   |  0.8202 |\n",
            "|     vowel      |    0.8242   |  0.7939 |\n",
            "|     vowel      |    0.8202   |  0.8040 |\n",
            "|     vowel      |    0.8485   |  0.8465 |\n",
            "|    soybean     |    0.9290   |  0.9231 |\n",
            "|    soybean     |    0.8991   |  0.8991 |\n",
            "|    soybean     |    0.9142   |  0.8905 |\n",
            "|    soybean     |    0.9199   |  0.9021 |\n",
            "|    soybean     |    0.9349   |  0.9231 |\n",
            "|    soybean     |    0.9021   |  0.8991 |\n",
            "|    soybean     |    0.9349   |  0.9290 |\n",
            "|    soybean     |    0.9288   |  0.9288 |\n",
            "|    soybean     |    0.9172   |  0.9320 |\n",
            "|    soybean     |    0.9139   |  0.9139 |\n",
            "|    segment     |    0.9697   |  0.9619 |\n",
            "|    segment     |    0.9671   |  0.9636 |\n",
            "|    segment     |    0.9697   |  0.9645 |\n",
            "|    segment     |    0.9688   |  0.9610 |\n",
            "|    segment     |    0.9723   |  0.9593 |\n",
            "|    segment     |    0.9740   |  0.9723 |\n",
            "|    segment     |    0.9688   |  0.9489 |\n",
            "|    segment     |    0.9714   |  0.9628 |\n",
            "|    segment     |    0.9636   |  0.9619 |\n",
            "|    segment     |    0.9671   |  0.9558 |\n",
            "|      vote      |    0.9495   |  0.9220 |\n",
            "|      vote      |    0.9539   |  0.9724 |\n",
            "|      vote      |    0.9541   |  0.9587 |\n",
            "|      vote      |    0.9585   |  0.9493 |\n",
            "|      vote      |    0.9220   |  0.9358 |\n",
            "|      vote      |    0.9816   |  0.9447 |\n",
            "|      vote      |    0.9495   |  0.9541 |\n",
            "|      vote      |    0.9401   |  0.9493 |\n",
            "|      vote      |    0.9587   |  0.9541 |\n",
            "|      vote      |    0.9677   |  0.9724 |\n",
            "|     sonar      |    0.7404   |  0.7308 |\n",
            "|     sonar      |    0.7788   |  0.7596 |\n",
            "|     sonar      |    0.7212   |  0.7404 |\n",
            "|     sonar      |    0.8173   |  0.7981 |\n",
            "|     sonar      |    0.8077   |  0.7788 |\n",
            "|     sonar      |    0.7981   |  0.6635 |\n",
            "|     sonar      |    0.7212   |  0.6923 |\n",
            "|     sonar      |    0.7019   |  0.7212 |\n",
            "|     sonar      |    0.7212   |  0.7404 |\n",
            "|     sonar      |    0.7692   |  0.7500 |\n",
            "|      zoo       |    1.0000   |  1.0000 |\n",
            "|      zoo       |    1.0000   |  1.0000 |\n",
            "|      zoo       |    0.9762   |  0.9286 |\n",
            "|      zoo       |    1.0000   |  1.0000 |\n",
            "|      zoo       |    1.0000   |  1.0000 |\n",
            "|      zoo       |    0.9762   |  1.0000 |\n",
            "|      zoo       |    1.0000   |  0.9762 |\n",
            "|      zoo       |    1.0000   |  1.0000 |\n",
            "|      zoo       |    1.0000   |  1.0000 |\n",
            "|      zoo       |    1.0000   |  1.0000 |\n",
            "|    vehicle     |    0.7541   |  0.7518 |\n",
            "|    vehicle     |    0.7470   |  0.6832 |\n",
            "|    vehicle     |    0.7447   |  0.6832 |\n",
            "|    vehicle     |    0.7778   |  0.6927 |\n",
            "|    vehicle     |    0.7423   |  0.7210 |\n",
            "|    vehicle     |    0.7801   |  0.6879 |\n",
            "|    vehicle     |    0.7210   |  0.6879 |\n",
            "|    vehicle     |    0.7589   |  0.7281 |\n",
            "|    vehicle     |    0.7376   |  0.7116 |\n",
            "|    vehicle     |    0.7541   |  0.6809 |\n",
            "|    waveform    |    0.8356   |  0.8004 |\n",
            "|    waveform    |    0.8340   |  0.8144 |\n",
            "|    waveform    |    0.8288   |  0.8000 |\n",
            "|    waveform    |    0.8416   |  0.8216 |\n",
            "|    waveform    |    0.8336   |  0.8104 |\n",
            "|    waveform    |    0.8232   |  0.8072 |\n",
            "|    waveform    |    0.8240   |  0.7980 |\n",
            "|    waveform    |    0.8336   |  0.8032 |\n",
            "|    waveform    |    0.8248   |  0.8028 |\n",
            "|    waveform    |    0.8328   |  0.7920 |\n",
            "| primary-tumor  |    0.3642   |  0.4371 |\n",
            "| primary-tumor  |    0.4371   |  0.4238 |\n",
            "| primary-tumor  |    0.3709   |  0.3510 |\n",
            "| primary-tumor  |    0.4503   |  0.3974 |\n",
            "| primary-tumor  |    0.4570   |  0.4305 |\n",
            "| primary-tumor  |    0.4040   |  0.3974 |\n",
            "| primary-tumor  |    0.4503   |  0.3377 |\n",
            "| primary-tumor  |    0.4172   |  0.4172 |\n",
            "| primary-tumor  |    0.5298   |  0.5364 |\n",
            "| primary-tumor  |    0.4371   |  0.4172 |\n",
            "|     splice     |    0.9448   |  0.9448 |\n",
            "|     splice     |    0.9448   |  0.9429 |\n",
            "|     splice     |    0.9517   |  0.9498 |\n",
            "|     splice     |    0.9517   |  0.9354 |\n",
            "|     splice     |    0.9511   |  0.9480 |\n",
            "|     splice     |    0.9423   |  0.9498 |\n",
            "|     splice     |    0.9448   |  0.9361 |\n",
            "|     splice     |    0.9542   |  0.9455 |\n",
            "|     splice     |    0.9524   |  0.9567 |\n",
            "|     splice     |    0.9429   |  0.9461 |\n",
            "|    ringnorm    |    0.9778   |  0.9322 |\n",
            "|    ringnorm    |    0.9754   |  0.9303 |\n",
            "|    ringnorm    |    0.9792   |  0.9249 |\n",
            "|    ringnorm    |    0.9808   |  0.9427 |\n",
            "|    ringnorm    |    0.9770   |  0.9305 |\n",
            "|    ringnorm    |    0.9784   |  0.9343 |\n",
            "|    ringnorm    |    0.9795   |  0.9389 |\n",
            "|    ringnorm    |    0.9800   |  0.9408 |\n",
            "|    ringnorm    |    0.9778   |  0.9368 |\n",
            "|    ringnorm    |    0.9786   |  0.9281 |\n",
            "|     col10      |    0.7822   |  0.7663 |\n",
            "|     col10      |    0.7760   |  0.7740 |\n",
            "|     col10      |    0.7743   |  0.7743 |\n",
            "|     col10      |    0.7641   |  0.7542 |\n",
            "|     col10      |    0.7614   |  0.7525 |\n",
            "|     col10      |    0.7691   |  0.7830 |\n",
            "|     col10      |    0.7663   |  0.7723 |\n",
            "|     col10      |    0.7909   |  0.7859 |\n",
            "|     col10      |    0.7802   |  0.7891 |\n",
            "|     col10      |    0.7948   |  0.8028 |\n",
            "|      cmc       |    0.4993   |  0.5278 |\n",
            "|      cmc       |    0.5367   |  0.5285 |\n",
            "|      cmc       |    0.5224   |  0.4803 |\n",
            "|      cmc       |    0.5204   |  0.4918 |\n",
            "|      cmc       |    0.5292   |  0.5427 |\n",
            "|      cmc       |    0.5041   |  0.5041 |\n",
            "|      cmc       |    0.5414   |  0.5034 |\n",
            "|      cmc       |    0.5231   |  0.5190 |\n",
            "|      cmc       |    0.5102   |  0.4830 |\n",
            "|      cmc       |    0.5272   |  0.5149 |\n",
            "|    column3C    |    0.8387   |  0.8258 |\n",
            "|    column3C    |    0.8581   |  0.8387 |\n",
            "|    column3C    |    0.8581   |  0.8323 |\n",
            "|    column3C    |    0.8452   |  0.8065 |\n",
            "|    column3C    |    0.8774   |  0.8581 |\n",
            "|    column3C    |    0.8258   |  0.7742 |\n",
            "|    column3C    |    0.8258   |  0.8000 |\n",
            "|    column3C    |    0.8194   |  0.8000 |\n",
            "|    column3C    |    0.8323   |  0.8452 |\n",
            "|    column3C    |    0.8581   |  0.8000 |\n",
            "|     ecoli      |    0.7927   |  0.8232 |\n",
            "|     ecoli      |    0.8712   |  0.8834 |\n",
            "|     ecoli      |    0.8841   |  0.8476 |\n",
            "|     ecoli      |    0.8650   |  0.8466 |\n",
            "|     ecoli      |    0.8171   |  0.8537 |\n",
            "|     ecoli      |    0.8098   |  0.7975 |\n",
            "|     ecoli      |    0.8476   |  0.8598 |\n",
            "|     ecoli      |    0.8773   |  0.8712 |\n",
            "|     ecoli      |    0.7988   |  0.7866 |\n",
            "|     ecoli      |    0.8834   |  0.8589 |\n",
            "|      monk      |    0.9016   |  0.9344 |\n",
            "|      monk      |    0.9016   |  0.9180 |\n",
            "|      monk      |    0.9016   |  0.9016 |\n",
            "|      monk      |    0.9672   |  0.9016 |\n",
            "|      monk      |    0.9016   |  0.9344 |\n",
            "|      monk      |    0.8689   |  0.8852 |\n",
            "|      monk      |    0.9180   |  0.8689 |\n",
            "|      monk      |    0.9016   |  0.9180 |\n",
            "|      monk      |    0.9508   |  0.9508 |\n",
            "|      monk      |    0.9016   |  0.8852 |\n",
            "|  transfusion   |    0.7727   |  0.7487 |\n",
            "|  transfusion   |    0.7487   |  0.7406 |\n",
            "|  transfusion   |    0.7193   |  0.7594 |\n",
            "|  transfusion   |    0.7380   |  0.7246 |\n",
            "|  transfusion   |    0.7567   |  0.7139 |\n",
            "|  transfusion   |    0.7326   |  0.7059 |\n",
            "|  transfusion   |    0.7513   |  0.7353 |\n",
            "|  transfusion   |    0.7166   |  0.7139 |\n",
            "|  transfusion   |    0.7112   |  0.7326 |\n",
            "|  transfusion   |    0.7620   |  0.7460 |\n",
            "| wineCultivars  |    0.9740   |  0.9870 |\n",
            "| wineCultivars  |    0.9474   |  0.8553 |\n",
            "| wineCultivars  |    0.9610   |  0.9740 |\n",
            "| wineCultivars  |    0.9737   |  0.9474 |\n",
            "| wineCultivars  |    0.9740   |  0.9740 |\n",
            "| wineCultivars  |    0.9474   |  0.8947 |\n",
            "| wineCultivars  |    0.9351   |  0.9481 |\n",
            "| wineCultivars  |    0.9737   |  0.9342 |\n",
            "| wineCultivars  |    0.9610   |  0.8571 |\n",
            "| wineCultivars  |    0.9605   |  0.9474 |\n",
            "|   hillValley   |    0.9967   |  0.4917 |\n",
            "|   hillValley   |    0.9637   |  0.5215 |\n",
            "|   hillValley   |    0.9505   |  0.5314 |\n",
            "|   hillValley   |    0.9835   |  0.5380 |\n",
            "|   hillValley   |    0.9736   |  0.4851 |\n",
            "|   hillValley   |    0.9802   |  0.4917 |\n",
            "|   hillValley   |    0.9835   |  0.5380 |\n",
            "|   hillValley   |    0.9835   |  0.5314 |\n",
            "|   hillValley   |    0.9703   |  0.5809 |\n",
            "|   hillValley   |    0.9868   |  0.4950 |\n",
            "| movementLibras |    0.7056   |  0.6278 |\n",
            "| movementLibras |    0.7222   |  0.6722 |\n",
            "| movementLibras |    0.7056   |  0.6500 |\n",
            "| movementLibras |    0.6667   |  0.6500 |\n",
            "| movementLibras |    0.7056   |  0.6222 |\n",
            "| movementLibras |    0.7222   |  0.6889 |\n",
            "| movementLibras |    0.7222   |  0.6167 |\n",
            "| movementLibras |    0.7556   |  0.6722 |\n",
            "| movementLibras |    0.6833   |  0.6222 |\n",
            "| movementLibras |    0.7000   |  0.5722 |\n",
            "|    spambase    |    0.9348   |  0.9222 |\n",
            "|    spambase    |    0.9135   |  0.9074 |\n",
            "|    spambase    |    0.9222   |  0.9135 |\n",
            "|    spambase    |    0.9300   |  0.9265 |\n",
            "|    spambase    |    0.9261   |  0.9235 |\n",
            "|    spambase    |    0.9257   |  0.9187 |\n",
            "|    spambase    |    0.9200   |  0.9174 |\n",
            "|    spambase    |    0.9239   |  0.9187 |\n",
            "|    spambase    |    0.9266   |  0.9109 |\n",
            "|    spambase    |    0.9270   |  0.9126 |\n",
            "|     yeast      |    0.5554   |  0.5568 |\n",
            "|     yeast      |    0.5765   |  0.5575 |\n",
            "|     yeast      |    0.5635   |  0.5243 |\n",
            "|     yeast      |    0.5710   |  0.5616 |\n",
            "|     yeast      |    0.6014   |  0.5716 |\n",
            "|     yeast      |    0.5440   |  0.5359 |\n",
            "|     yeast      |    0.5973   |  0.5797 |\n",
            "|     yeast      |    0.5535   |  0.5697 |\n",
            "|     yeast      |    0.5527   |  0.5473 |\n",
            "|     yeast      |    0.5724   |  0.5670 |\n",
            "|  iphonetweets  |    0.7293   |  0.7707 |\n",
            "|  iphonetweets  |    0.7782   |  0.7820 |\n",
            "|  iphonetweets  |    0.7331   |  0.7519 |\n",
            "|  iphonetweets  |    0.7218   |  0.7406 |\n",
            "|  iphonetweets  |    0.7481   |  0.7744 |\n",
            "|  iphonetweets  |    0.7331   |  0.7669 |\n",
            "|  iphonetweets  |    0.7406   |  0.7406 |\n",
            "|  iphonetweets  |    0.7707   |  0.7519 |\n",
            "|  iphonetweets  |    0.7105   |  0.7256 |\n",
            "|  iphonetweets  |    0.7820   |  0.7895 |\n",
            "|  hobbittweets  |    0.9272   |  0.9272 |\n",
            "|  hobbittweets  |    0.9310   |  0.9425 |\n",
            "|  hobbittweets  |    0.9119   |  0.9157 |\n",
            "|  hobbittweets  |    0.9387   |  0.9464 |\n",
            "|  hobbittweets  |    0.9349   |  0.9349 |\n",
            "|  hobbittweets  |    0.9272   |  0.9234 |\n",
            "|  hobbittweets  |    0.9349   |  0.9387 |\n",
            "|  hobbittweets  |    0.9387   |  0.9387 |\n",
            "|  hobbittweets  |    0.9502   |  0.9502 |\n",
            "|  hobbittweets  |    0.9234   |  0.9234 |\n",
            "+----------------+-------------+---------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpsfLmKyUTCl"
      },
      "source": [
        "np.save(ds_folderpath + \"/\" + 'bagging.npy',foldPreds_Bagging ) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAgsYLf4qmK_"
      },
      "source": [
        "Sklearn Adaboost Fonksiyonları\r\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW3mwZeYee5S"
      },
      "source": [
        "from abc import ABCMeta, abstractmethod\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from scipy.special import xlogy\r\n",
        "\r\n",
        "from sklearn.ensemble import BaseEnsemble\r\n",
        "from sklearn.base import ClassifierMixin, RegressorMixin, is_classifier, is_regressor\r\n",
        "\r\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\r\n",
        "from sklearn.utils import check_array, check_random_state, _safe_indexing\r\n",
        "from sklearn.utils.extmath import softmax, stable_cumsum\r\n",
        "from sklearn.metrics import accuracy_score, r2_score\r\n",
        "from sklearn.utils.validation import check_is_fitted, _check_sample_weight,has_fit_parameter,_num_samples, _deprecate_positional_args\r\n",
        "\r\n",
        "# __all__ = [\r\n",
        "#     'AdaBoostClassifier',\r\n",
        "#     'AdaBoostRegressor',\r\n",
        "# ]\r\n",
        "\r\n",
        "class BaseWeightBoosting_IMP(BaseEnsemble, metaclass=ABCMeta):\r\n",
        "    @abstractmethod\r\n",
        "    def __init__(self,\r\n",
        "                 base_estimator=None, *,\r\n",
        "                 n_estimators=50,\r\n",
        "                 estimator_params=tuple(),\r\n",
        "                 learning_rate=1.,\r\n",
        "                 random_state=None):\r\n",
        "\r\n",
        "        super().__init__(\r\n",
        "            base_estimator=base_estimator,\r\n",
        "            n_estimators=n_estimators,\r\n",
        "            estimator_params=estimator_params)\r\n",
        "\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.random_state = random_state\r\n",
        "\r\n",
        "    def _check_X(self, X):\r\n",
        "        return check_array(X, accept_sparse=['csr', 'csc'], ensure_2d=True,\r\n",
        "                           allow_nd=True, dtype=None)\r\n",
        "\r\n",
        "    def fit(self, X, X_test, y, sample_weight=None):\r\n",
        "        # Check parameters\r\n",
        "        if self.learning_rate <= 0:\r\n",
        "            raise ValueError(\"learning_rate must be greater than zero\")\r\n",
        "      #  print(\"_2_\")\r\n",
        "        # X, y = self._validate_data(X, y,\r\n",
        "        #                            accept_sparse=['csr', 'csc'],\r\n",
        "        #                            ensure_2d=True,\r\n",
        "        #                            allow_nd=True,\r\n",
        "        #                            dtype=None,\r\n",
        "        #                            y_numeric=is_regressor(self))\r\n",
        "        #print(\"_3_\")\r\n",
        "\r\n",
        "        sample_weight = _check_sample_weight(sample_weight, X, np.float64)\r\n",
        "        sample_weight /= sample_weight.sum()\r\n",
        "        if np.any(sample_weight < 0):\r\n",
        "            raise ValueError(\"sample_weight cannot contain negative weights\")\r\n",
        "        #print(\"_4_\")\r\n",
        "        # Check parameters\r\n",
        "        self._validate_estimator()\r\n",
        "\r\n",
        "        # Clear any previous fit results\r\n",
        "        self.estimators_ = []\r\n",
        "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\r\n",
        "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\r\n",
        "        #print(\"_5_\")\r\n",
        "        # Initializion of the random number instance that will be used to\r\n",
        "        # generate a seed at each iteration\r\n",
        "        random_state = check_random_state(self.random_state)\r\n",
        "        #print(\"_6_\")\r\n",
        "        c = 0\r\n",
        "        for iboost in range(self.n_estimators):\r\n",
        "            c+=1\r\n",
        "           # print(\"_7_\", str(c))\r\n",
        "            # dfx = pd.DataFrame(data=X)\r\n",
        "            # dfy = pd.DataFrame(data=y)\r\n",
        "            # dfx_test = pd.DataFrame(data=X_test)\r\n",
        "           # print(\"_7_a\", str(c))\r\n",
        "\r\n",
        "            # print(dfx.values.shape)\r\n",
        "            # print(dfy.values.shape)\r\n",
        "            # print(dfx_test.values.shape)\r\n",
        "            # print(type(X))\r\n",
        "            # print(type(y))\r\n",
        "\r\n",
        "            #HER TEKIL AGAC OLUSTURULMADAN ONCE IMPROVED SPACE\r\n",
        "            imp_tr, imp_ts = generate_imp_space(X, y, X_test, imp_feature_size=1, foz=4)\r\n",
        "           # print(\"_7_b\", str(c))\r\n",
        "            # Boosting step\r\n",
        "            sample_weight, estimator_weight, estimator_error = self._boost(\r\n",
        "                iboost,\r\n",
        "                imp_tr, imp_ts, y,\r\n",
        "                sample_weight,\r\n",
        "                random_state)\r\n",
        "           # print(\"_7_c\", str(c))\r\n",
        "            # Early termination\r\n",
        "            if sample_weight is None:\r\n",
        "                break\r\n",
        "\r\n",
        "            self.estimator_weights_[iboost] = estimator_weight\r\n",
        "            self.estimator_errors_[iboost] = estimator_error\r\n",
        "\r\n",
        "            # Stop if error is zero\r\n",
        "            if estimator_error == 0:\r\n",
        "                break\r\n",
        "\r\n",
        "            sample_weight_sum = np.sum(sample_weight)\r\n",
        "\r\n",
        "            # Stop if the sum of sample weights has become non-positive\r\n",
        "            if sample_weight_sum <= 0:\r\n",
        "                break\r\n",
        "\r\n",
        "            if iboost < self.n_estimators - 1:\r\n",
        "                # Normalize\r\n",
        "                sample_weight /= sample_weight_sum\r\n",
        "\r\n",
        "        return self\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def _boost(self, iboost, X, y, sample_weight, random_state):\r\n",
        "       # print(\"_8_\")\r\n",
        "        pass\r\n",
        "\r\n",
        "    def staged_score(self, X, y, sample_weight=None):\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        for y_pred in self.staged_predict(X):\r\n",
        "            if is_classifier(self):\r\n",
        "                yield accuracy_score(y, y_pred, sample_weight=sample_weight)\r\n",
        "            else:\r\n",
        "                yield r2_score(y, y_pred, sample_weight=sample_weight)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def feature_importances_(self):\r\n",
        "        if self.estimators_ is None or len(self.estimators_) == 0:\r\n",
        "            raise ValueError(\"Estimator not fitted, \"\r\n",
        "                             \"call `fit` before `feature_importances_`.\")\r\n",
        "\r\n",
        "        try:\r\n",
        "            norm = self.estimator_weights_.sum()\r\n",
        "            return (sum(weight * clf.feature_importances_ for weight, clf\r\n",
        "                    in zip(self.estimator_weights_, self.estimators_))\r\n",
        "                    / norm)\r\n",
        "\r\n",
        "        except AttributeError as e:\r\n",
        "            raise AttributeError(\r\n",
        "                \"Unable to compute feature importances \"\r\n",
        "                \"since base_estimator does not have a \"\r\n",
        "                \"feature_importances_ attribute\") from e\r\n",
        "\r\n",
        "\r\n",
        "    def _samme_proba(self, estimator, n_classes, X):\r\n",
        "        proba = estimator.predict_proba(X)\r\n",
        "\r\n",
        "        # Displace zero probabilities so the log is defined.\r\n",
        "        # Also fix negative elements which may occur with\r\n",
        "        # negative sample weights.\r\n",
        "        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\r\n",
        "        log_proba = np.log(proba)\r\n",
        "\r\n",
        "        return (n_classes - 1) * (log_proba - (1. / n_classes)\r\n",
        "                                  * log_proba.sum(axis=1)[:, np.newaxis])\r\n",
        "\r\n",
        "\r\n",
        "################################################  ADABOOST  ########################################################################\r\n",
        "\r\n",
        "class AdaBoostClassifier_IMP(ClassifierMixin, BaseWeightBoosting_IMP):\r\n",
        "    @_deprecate_positional_args\r\n",
        "    def __init__(self,\r\n",
        "                 base_estimator=None, *,\r\n",
        "                 n_estimators=50,\r\n",
        "                 learning_rate=1.,\r\n",
        "                 algorithm='SAMME.R',\r\n",
        "                 testpredictions = [],\r\n",
        "                 random_state=None):\r\n",
        "\r\n",
        "        super().__init__(\r\n",
        "            base_estimator=base_estimator,\r\n",
        "            n_estimators=n_estimators,\r\n",
        "            learning_rate=learning_rate,\r\n",
        "            random_state=random_state)\r\n",
        "\r\n",
        "        self.algorithm = algorithm\r\n",
        "        self.testpredictions = testpredictions\r\n",
        "\r\n",
        "    def fit(self, X, X_test, y, sample_weight=None):\r\n",
        "        # Check that algorithm is supported\r\n",
        "        if self.algorithm not in ('SAMME', 'SAMME.R'):\r\n",
        "            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\r\n",
        "      #  print(\"_1_\")\r\n",
        "        # Fit\r\n",
        "        self.testpredictions = []\r\n",
        "        return super().fit(X, X_test, y, sample_weight)\r\n",
        "\r\n",
        "    def _validate_estimator(self):\r\n",
        "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\r\n",
        "        super()._validate_estimator(\r\n",
        "            default=DecisionTreeClassifier(max_depth=1))\r\n",
        "\r\n",
        "        #  SAMME-R requires predict_proba-enabled base estimators\r\n",
        "        if self.algorithm == 'SAMME.R':\r\n",
        "            if not hasattr(self.base_estimator_, 'predict_proba'):\r\n",
        "                raise TypeError(\r\n",
        "                    \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\r\n",
        "                    \"that the weak learner supports the calculation of class \"\r\n",
        "                    \"probabilities with a predict_proba method.\\n\"\r\n",
        "                    \"Please change the base estimator or set \"\r\n",
        "                    \"algorithm='SAMME' instead.\")\r\n",
        "        if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\r\n",
        "            raise ValueError(\"%s doesn't support sample_weight.\"\r\n",
        "                             % self.base_estimator_.__class__.__name__)\r\n",
        "\r\n",
        "    def _boost(self, iboost, X, X_test, y, sample_weight, random_state):\r\n",
        "        if self.algorithm == 'SAMME.R':\r\n",
        "       #     print(\"_9_1_\")\r\n",
        "            return self._boost_real(iboost, X, X_test, y, sample_weight, random_state)\r\n",
        "\r\n",
        "        else:  # elif self.algorithm == \"SAMME\":\r\n",
        "        #    print(\"_9_2_\")\r\n",
        "            return self._boost_discrete(iboost, X, X_test, y, sample_weight,\r\n",
        "                                        random_state)\r\n",
        "\r\n",
        "    def _boost_real(self, iboost, X, X_test, y, sample_weight, random_state):\r\n",
        "        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\r\n",
        "        estimator = self._make_estimator(random_state=random_state)\r\n",
        "\r\n",
        "        estimator.fit(X, y, sample_weight=sample_weight)\r\n",
        "\r\n",
        "        y_predict_proba = estimator.predict_proba(X)\r\n",
        "\r\n",
        "        if iboost == 0:\r\n",
        "            self.classes_ = getattr(estimator, 'classes_', None)\r\n",
        "            self.n_classes_ = len(self.classes_)\r\n",
        "\r\n",
        "        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\r\n",
        "                                       axis=0)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_[:, np.newaxis]\r\n",
        "\r\n",
        "        if self.algorithm == 'SAMME.R':#\r\n",
        "            preds = super()._samme_proba(estimator, n_classes, X_test)\r\n",
        "            self.testpredictions.append(preds) #\r\n",
        "            # The weights are all 1. for SAMME.R\r\n",
        "            # pred = sum(_samme_proba(estimator, n_classes, X)\r\n",
        "            #             for estimator in self.estimators_)\r\n",
        "        else:  # self.algorithm == \"SAMME\" #\r\n",
        "            self.testpredictions.append(estimator.predict(X_test)) #\r\n",
        "            # pred = sum((estimator.predict(X) == classes).T * w\r\n",
        "            #             for estimator, w in zip(self.estimators_,\r\n",
        "            #                                     self.estimator_weights_))\r\n",
        "            \r\n",
        "        # Instances incorrectly classified\r\n",
        "        incorrect = y_predict != y\r\n",
        "\r\n",
        "        # Error fraction\r\n",
        "        estimator_error = np.mean(\r\n",
        "            np.average(incorrect, weights=sample_weight, axis=0))\r\n",
        "\r\n",
        "        # Stop if classification is perfect\r\n",
        "        if estimator_error <= 0:\r\n",
        "            return sample_weight, 1., 0.\r\n",
        "\r\n",
        "        # Construct y coding as described in Zhu et al [2]:\r\n",
        "        #\r\n",
        "        #    y_k = 1 if c == k else -1 / (K - 1)\r\n",
        "        #\r\n",
        "        # where K == n_classes_ and c, k in [0, K) are indices along the second\r\n",
        "        # axis of the y coding with c being the index corresponding to the true\r\n",
        "        # class label.\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_\r\n",
        "        y_codes = np.array([-1. / (n_classes - 1), 1.])\r\n",
        "        y_coding = y_codes.take(classes == y[:, np.newaxis])\r\n",
        "\r\n",
        "        # Displace zero probabilities so the log is defined.\r\n",
        "        # Also fix negative elements which may occur with\r\n",
        "        # negative sample weights.\r\n",
        "        proba = y_predict_proba  # alias for readability\r\n",
        "        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\r\n",
        "\r\n",
        "        # Boost weight using multi-class AdaBoost SAMME.R alg\r\n",
        "        estimator_weight = (-1. * self.learning_rate\r\n",
        "                            * ((n_classes - 1.) / n_classes)\r\n",
        "                            * xlogy(y_coding, y_predict_proba).sum(axis=1))\r\n",
        "\r\n",
        "        # Only boost the weights if it will fit again\r\n",
        "        if not iboost == self.n_estimators - 1:\r\n",
        "            # Only boost positive weights\r\n",
        "            sample_weight *= np.exp(estimator_weight *\r\n",
        "                                    ((sample_weight > 0) |\r\n",
        "                                     (estimator_weight < 0)))\r\n",
        "\r\n",
        "        return sample_weight, 1., estimator_error\r\n",
        "\r\n",
        "    def _boost_discrete(self, iboost, X, X_test, y, sample_weight, random_state):\r\n",
        "        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\r\n",
        "        estimator = self._make_estimator(random_state=random_state)\r\n",
        "\r\n",
        "        estimator.fit(X, y, sample_weight=sample_weight)\r\n",
        "\r\n",
        "        y_predict = estimator.predict(X)\r\n",
        "\r\n",
        "        test_predict_proba = estimator.predict_proba(X_test) #\r\n",
        "        test_predict = self.classes_.take(np.argmax(test_predict_proba, axis=1),\r\n",
        "                                       axis=0) #\r\n",
        "        self.predictions.append(test_predict)#\r\n",
        "\r\n",
        "\r\n",
        "        if iboost == 0:\r\n",
        "            self.classes_ = getattr(estimator, 'classes_', None)\r\n",
        "            self.n_classes_ = len(self.classes_)\r\n",
        "\r\n",
        "        # Instances incorrectly classified\r\n",
        "        incorrect = y_predict != y\r\n",
        "\r\n",
        "        # Error fraction\r\n",
        "        estimator_error = np.mean(\r\n",
        "            np.average(incorrect, weights=sample_weight, axis=0))\r\n",
        "\r\n",
        "        # Stop if classification is perfect\r\n",
        "        if estimator_error <= 0:\r\n",
        "            return sample_weight, 1., 0.\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "\r\n",
        "        # Stop if the error is at least as bad as random guessing\r\n",
        "        if estimator_error >= 1. - (1. / n_classes):\r\n",
        "            self.estimators_.pop(-1)\r\n",
        "            if len(self.estimators_) == 0:\r\n",
        "                raise ValueError('BaseClassifier in AdaBoostClassifier '\r\n",
        "                                 'ensemble is worse than random, ensemble '\r\n",
        "                                 'can not be fit.')\r\n",
        "            return None, None, None\r\n",
        "\r\n",
        "        # Boost weight using multi-class AdaBoost SAMME alg\r\n",
        "        estimator_weight = self.learning_rate * (\r\n",
        "            np.log((1. - estimator_error) / estimator_error) +\r\n",
        "            np.log(n_classes - 1.))\r\n",
        "\r\n",
        "        # Only boost the weights if I will fit again\r\n",
        "        if not iboost == self.n_estimators - 1:\r\n",
        "            # Only boost positive weights\r\n",
        "            sample_weight *= np.exp(estimator_weight * incorrect *\r\n",
        "                                    (sample_weight > 0))\r\n",
        "\r\n",
        "        return sample_weight, estimator_weight, estimator_error\r\n",
        "\r\n",
        "    def predict(self, X):\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        pred = self.decision_function(X)\r\n",
        "\r\n",
        "        if self.n_classes_ == 2:\r\n",
        "            return self.classes_.take(pred > 0, axis=0)\r\n",
        "\r\n",
        "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\r\n",
        "    \r\n",
        "    def predict(self): #\r\n",
        "        # X = self._check_X(X)\r\n",
        "\r\n",
        "        pred = self.decision_function_test()\r\n",
        "\r\n",
        "        if self.n_classes_ == 2:\r\n",
        "            return self.classes_.take(pred > 0, axis=0)\r\n",
        "\r\n",
        "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\r\n",
        "\r\n",
        "    def staged_predict(self, X):\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_\r\n",
        "\r\n",
        "        if n_classes == 2:\r\n",
        "            for pred in self.staged_decision_function(X):\r\n",
        "                yield np.array(classes.take(pred > 0, axis=0))\r\n",
        "\r\n",
        "        else:\r\n",
        "            for pred in self.staged_decision_function(X):\r\n",
        "                yield np.array(classes.take(\r\n",
        "                    np.argmax(pred, axis=1), axis=0))\r\n",
        "                \r\n",
        "    def decision_function_test(self): #\r\n",
        "      check_is_fitted(self)\r\n",
        "      # X = self._check_X(X)\r\n",
        "\r\n",
        "      n_classes = self.n_classes_\r\n",
        "      classes = self.classes_[:, np.newaxis]\r\n",
        "\r\n",
        "      if self.algorithm == 'SAMME.R':\r\n",
        "          pred = sum(predicts for predicts in self.testpredictions)\r\n",
        "      else:\r\n",
        "          pred = sum( (predicts == classes).T * w \r\n",
        "                     for predicts, w in zip(self.testpredictions, \r\n",
        "                                            self.estimator_weights_))\r\n",
        "      # if self.algorithm == 'SAMME.R':\r\n",
        "      #     # The weights are all 1. for SAMME.R\r\n",
        "      #     pred = sum(_samme_proba(estimator, n_classes, X)\r\n",
        "      #                 for estimator in self.estimators_)\r\n",
        "      # else:  # self.algorithm == \"SAMME\"\r\n",
        "      #     pred = sum((estimator.predict(X) == classes).T * w\r\n",
        "      #                 for estimator, w in zip(self.estimators_,\r\n",
        "      #                                         self.estimator_weights_))\r\n",
        "\r\n",
        "      pred /= self.estimator_weights_.sum()\r\n",
        "      if n_classes == 2:\r\n",
        "          pred[:, 0] *= -1\r\n",
        "          return pred.sum(axis=1)\r\n",
        "      return pred\r\n",
        "\r\n",
        "\r\n",
        "    def decision_function(self, X):\r\n",
        "        check_is_fitted(self)\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_[:, np.newaxis]\r\n",
        "\r\n",
        "        if self.algorithm == 'SAMME.R':\r\n",
        "            # The weights are all 1. for SAMME.R\r\n",
        "            pred = sum(_samme_proba(estimator, n_classes, X)\r\n",
        "                       for estimator in self.estimators_)\r\n",
        "        else:  # self.algorithm == \"SAMME\"\r\n",
        "            pred = sum((estimator.predict(X) == classes).T * w\r\n",
        "                       for estimator, w in zip(self.estimators_,\r\n",
        "                                               self.estimator_weights_))\r\n",
        "\r\n",
        "        pred /= self.estimator_weights_.sum()\r\n",
        "        if n_classes == 2:\r\n",
        "            pred[:, 0] *= -1\r\n",
        "            return pred.sum(axis=1)\r\n",
        "        return pred\r\n",
        "\r\n",
        "    def staged_decision_function(self, X):\r\n",
        "        check_is_fitted(self)\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "        classes = self.classes_[:, np.newaxis]\r\n",
        "        pred = None\r\n",
        "        norm = 0.\r\n",
        "\r\n",
        "        for weight, estimator in zip(self.estimator_weights_,\r\n",
        "                                     self.estimators_):\r\n",
        "            norm += weight\r\n",
        "\r\n",
        "            if self.algorithm == 'SAMME.R':\r\n",
        "                # The weights are all 1. for SAMME.R\r\n",
        "                current_pred = _samme_proba(estimator, n_classes, X)\r\n",
        "            else:  # elif self.algorithm == \"SAMME\":\r\n",
        "                current_pred = estimator.predict(X)\r\n",
        "                current_pred = (current_pred == classes).T * weight\r\n",
        "\r\n",
        "            if pred is None:\r\n",
        "                pred = current_pred\r\n",
        "            else:\r\n",
        "                pred += current_pred\r\n",
        "\r\n",
        "            if n_classes == 2:\r\n",
        "                tmp_pred = np.copy(pred)\r\n",
        "                tmp_pred[:, 0] *= -1\r\n",
        "                yield (tmp_pred / norm).sum(axis=1)\r\n",
        "            else:\r\n",
        "                yield pred / norm\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def _compute_proba_from_decision(decision, n_classes):\r\n",
        "        if n_classes == 2:\r\n",
        "            decision = np.vstack([-decision, decision]).T / 2\r\n",
        "        else:\r\n",
        "            decision /= (n_classes - 1)\r\n",
        "        return softmax(decision, copy=False)\r\n",
        "\r\n",
        "    def predict_proba(self, X):\r\n",
        "        check_is_fitted(self)\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "\r\n",
        "        if n_classes == 1:\r\n",
        "            return np.ones((_num_samples(X), 1))\r\n",
        "\r\n",
        "        decision = self.decision_function(X)\r\n",
        "        return self._compute_proba_from_decision(decision, n_classes)\r\n",
        "\r\n",
        "    def staged_predict_proba(self, X):\r\n",
        "        X = self._check_X(X)\r\n",
        "\r\n",
        "        n_classes = self.n_classes_\r\n",
        "\r\n",
        "        for decision in self.staged_decision_function(X):\r\n",
        "            yield self._compute_proba_from_decision(decision, n_classes)\r\n",
        "\r\n",
        "    def predict_log_proba(self, X):\r\n",
        "        X = self._check_X(X)\r\n",
        "        return np.log(self.predict_proba(X))\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw1YaJWEsciw"
      },
      "source": [
        "Adaboost\r\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWTgR3HrWXLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c57f18-d5b2-4209-93ca-f99b0350b92e"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\r\n",
        "from sklearn.model_selection import RepeatedKFold\r\n",
        "import warnings\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "foz=4\r\n",
        "imp_feature_size=1\r\n",
        "n_estimators = 10\r\n",
        "#Table\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "pt_adaboost = PrettyTable()\r\n",
        "\r\n",
        "pt_adaboost.field_names = [\"Dataset\", \"Imp_Adaboost\", \"Adaboost\"]\r\n",
        "\r\n",
        "foldPreds_AB = {}\r\n",
        "K =0\r\n",
        "for ds in datasets:\r\n",
        "  K+=1\r\n",
        "  rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=2652124)\r\n",
        "  if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "    df = datasets[ds]\r\n",
        "    X = df['text']\r\n",
        "    Y = df['label']\r\n",
        "  else:\r\n",
        "    X,Y = datasets[ds]\r\n",
        "    Y = Y.str.decode(\"utf-8\")\r\n",
        "    \r\n",
        "  # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\r\n",
        "  print(ds)\r\n",
        "  accuracies = []\r\n",
        "  accuracies_imp = []  \r\n",
        "  for train_index, test_index in rkf.split(X):\r\n",
        "\r\n",
        "    X_train = X.iloc[train_index]\r\n",
        "    X_test = X.iloc[test_index]\r\n",
        "    Y_train = Y.iloc[train_index]\r\n",
        "    Y_test = Y.iloc[test_index]\r\n",
        "\r\n",
        "    if ds == \"iphonetweets\" or ds==\"hobbittweets\":\r\n",
        "      vectorizer = CountVectorizer(analyzer=preprocess)\r\n",
        "      vectorizer.fit(X_train)\r\n",
        "\r\n",
        "      X_train = vectorizer.transform(X_train).toarray()\r\n",
        "      X_test = vectorizer.transform(X_test).toarray()\r\n",
        "      X_train = pd.DataFrame(X_train)\r\n",
        "      X_test = pd.DataFrame(X_test)\r\n",
        "\r\n",
        "    d = len(X_train.columns)\r\n",
        "    num_class = len(Y_train.value_counts())\r\n",
        "    \r\n",
        "    ab_clf = AdaBoostClassifier(n_estimators=10)\r\n",
        "    ab_clf.fit(X=X_train,y=Y_train)\r\n",
        "    y_preds = ab_clf.predict(X_test)\r\n",
        "\r\n",
        "    # print(\"-----------------------------------------------------------\")\r\n",
        "    ab_clf_imp = AdaBoostClassifier_IMP(n_estimators=10)\r\n",
        "    ab_clf_imp.fit(X=X_train, X_test=X_test , y=Y_train)\r\n",
        "    y_preds_imp = ab_clf_imp.predict()\r\n",
        "\r\n",
        "    # print(accuracy_score(Y_test,y_preds))\r\n",
        "    # print(accuracy_score(Y_test,y_preds_imp))\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    # print(\"--------------  {}  ----------------\".format(ds.upper()))\r\n",
        "    accuracies.append(accuracy_score(Y_test,y_preds))\r\n",
        "    accuracies_imp.append(accuracy_score(Y_test,y_preds_imp))\r\n",
        "    # print(\" \")\r\n",
        "    # print(\" \")\r\n",
        "    pt_adaboost.add_row((ds, \"%.4f\" % accuracy_score(Y_test,y_preds_imp), \"%.4f\" % accuracy_score(Y_test,y_preds)))\r\n",
        "\r\n",
        "  foldPreds_AB[ds] = accuracies\r\n",
        "  foldPreds_AB[ds+\"_imp\"] = accuracies_imp\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ionosphere\n",
            "diabetes\n",
            "glass\n",
            "abalone\n",
            "heart-statlog\n",
            "colic\n",
            "credit-g\n",
            "balance-scale\n",
            "autos\n",
            "credit-a\n",
            "breast-w\n",
            "breast-cancer\n",
            "audiology\n",
            "lymph\n",
            "hepatitis\n",
            "labor\n",
            "iris\n",
            "letter\n",
            "vowel\n",
            "soybean\n",
            "segment\n",
            "vote\n",
            "sonar\n",
            "zoo\n",
            "vehicle\n",
            "waveform\n",
            "primary-tumor\n",
            "splice\n",
            "ringnorm\n",
            "col10\n",
            "cmc\n",
            "column3C\n",
            "ecoli\n",
            "monk\n",
            "transfusion\n",
            "wineCultivars\n",
            "hillValley\n",
            "movementLibras\n",
            "spambase\n",
            "yeast\n",
            "iphonetweets\n",
            "hobbittweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr-LjV01W0v8",
        "outputId": "1d4238ee-9892-439a-d32c-6c78b688c2e0"
      },
      "source": [
        "print(pt_adaboost)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------+----------+\n",
            "|    Dataset     | Imp_Adaboost | Adaboost |\n",
            "+----------------+--------------+----------+\n",
            "|   ionosphere   |    0.9318    |  0.8920  |\n",
            "|   ionosphere   |    0.9086    |  0.9029  |\n",
            "|   ionosphere   |    0.9318    |  0.9318  |\n",
            "|   ionosphere   |    0.8914    |  0.8514  |\n",
            "|   ionosphere   |    0.9261    |  0.9261  |\n",
            "|   ionosphere   |    0.8857    |  0.8743  |\n",
            "|   ionosphere   |    0.9261    |  0.9432  |\n",
            "|   ionosphere   |    0.9200    |  0.8971  |\n",
            "|   ionosphere   |    0.9261    |  0.9091  |\n",
            "|   ionosphere   |    0.9200    |  0.9200  |\n",
            "|    diabetes    |    0.7318    |  0.7396  |\n",
            "|    diabetes    |    0.7370    |  0.7135  |\n",
            "|    diabetes    |    0.7891    |  0.7578  |\n",
            "|    diabetes    |    0.7526    |  0.7500  |\n",
            "|    diabetes    |    0.7708    |  0.7552  |\n",
            "|    diabetes    |    0.7135    |  0.7214  |\n",
            "|    diabetes    |    0.7240    |  0.7188  |\n",
            "|    diabetes    |    0.7656    |  0.7682  |\n",
            "|    diabetes    |    0.7578    |  0.7448  |\n",
            "|    diabetes    |    0.7526    |  0.7057  |\n",
            "|     glass      |    0.5728    |  0.5825  |\n",
            "|     glass      |    0.4706    |  0.4510  |\n",
            "|     glass      |    0.4660    |  0.3398  |\n",
            "|     glass      |    0.5000    |  0.4902  |\n",
            "|     glass      |    0.5049    |  0.3689  |\n",
            "|     glass      |    0.4706    |  0.4020  |\n",
            "|     glass      |    0.5146    |  0.4078  |\n",
            "|     glass      |    0.5294    |  0.5000  |\n",
            "|     glass      |    0.3981    |  0.3495  |\n",
            "|     glass      |    0.5784    |  0.4020  |\n",
            "|    abalone     |    0.1911    |  0.2282  |\n",
            "|    abalone     |    0.2288    |  0.1883  |\n",
            "|    abalone     |    0.2205    |  0.2325  |\n",
            "|    abalone     |    0.1989    |  0.2437  |\n",
            "|    abalone     |    0.2253    |  0.1984  |\n",
            "|    abalone     |    0.2360    |  0.2245  |\n",
            "|    abalone     |    0.2412    |  0.2138  |\n",
            "|    abalone     |    0.2269    |  0.2168  |\n",
            "|    abalone     |    0.1931    |  0.2532  |\n",
            "|    abalone     |    0.2375    |  0.2259  |\n",
            "| heart-statlog  |    0.8000    |  0.7556  |\n",
            "| heart-statlog  |    0.8074    |  0.8148  |\n",
            "| heart-statlog  |    0.7778    |  0.7630  |\n",
            "| heart-statlog  |    0.8074    |  0.8074  |\n",
            "| heart-statlog  |    0.7259    |  0.7407  |\n",
            "| heart-statlog  |    0.8000    |  0.8370  |\n",
            "| heart-statlog  |    0.7926    |  0.8074  |\n",
            "| heart-statlog  |    0.7778    |  0.7704  |\n",
            "| heart-statlog  |    0.8148    |  0.7926  |\n",
            "| heart-statlog  |    0.7556    |  0.8370  |\n",
            "|     colic      |    0.8207    |  0.8207  |\n",
            "|     colic      |    0.7663    |  0.7935  |\n",
            "|     colic      |    0.8098    |  0.7880  |\n",
            "|     colic      |    0.7880    |  0.8043  |\n",
            "|     colic      |    0.7935    |  0.8261  |\n",
            "|     colic      |    0.7609    |  0.7663  |\n",
            "|     colic      |    0.7554    |  0.7663  |\n",
            "|     colic      |    0.7880    |  0.7554  |\n",
            "|     colic      |    0.7609    |  0.7663  |\n",
            "|     colic      |    0.7609    |  0.7935  |\n",
            "|    credit-g    |    0.7000    |  0.7060  |\n",
            "|    credit-g    |    0.7040    |  0.7400  |\n",
            "|    credit-g    |    0.6880    |  0.6900  |\n",
            "|    credit-g    |    0.6960    |  0.7180  |\n",
            "|    credit-g    |    0.7180    |  0.7000  |\n",
            "|    credit-g    |    0.7260    |  0.7300  |\n",
            "|    credit-g    |    0.6900    |  0.7140  |\n",
            "|    credit-g    |    0.7240    |  0.7220  |\n",
            "|    credit-g    |    0.7080    |  0.7220  |\n",
            "|    credit-g    |    0.7160    |  0.6980  |\n",
            "| balance-scale  |    0.8818    |  0.8626  |\n",
            "| balance-scale  |    0.9071    |  0.8750  |\n",
            "| balance-scale  |    0.9010    |  0.8658  |\n",
            "| balance-scale  |    0.7628    |  0.8301  |\n",
            "| balance-scale  |    0.8946    |  0.8435  |\n",
            "| balance-scale  |    0.7083    |  0.8526  |\n",
            "| balance-scale  |    0.9297    |  0.8435  |\n",
            "| balance-scale  |    0.8590    |  0.8269  |\n",
            "| balance-scale  |    0.7732    |  0.8562  |\n",
            "| balance-scale  |    0.6026    |  0.8622  |\n",
            "|     autos      |    0.4752    |  0.3663  |\n",
            "|     autos      |    0.3267    |  0.5644  |\n",
            "|     autos      |    0.4257    |  0.3762  |\n",
            "|     autos      |    0.4752    |  0.3168  |\n",
            "|     autos      |    0.3960    |  0.4950  |\n",
            "|     autos      |    0.4554    |  0.4653  |\n",
            "|     autos      |    0.5248    |  0.4356  |\n",
            "|     autos      |    0.4851    |  0.4455  |\n",
            "|     autos      |    0.4851    |  0.3564  |\n",
            "|     autos      |    0.4554    |  0.4455  |\n",
            "|    credit-a    |    0.8464    |  0.8522  |\n",
            "|    credit-a    |    0.8609    |  0.8319  |\n",
            "|    credit-a    |    0.8551    |  0.8609  |\n",
            "|    credit-a    |    0.8406    |  0.8319  |\n",
            "|    credit-a    |    0.8435    |  0.8348  |\n",
            "|    credit-a    |    0.7971    |  0.8174  |\n",
            "|    credit-a    |    0.8377    |  0.8406  |\n",
            "|    credit-a    |    0.8435    |  0.8261  |\n",
            "|    credit-a    |    0.8493    |  0.8377  |\n",
            "|    credit-a    |    0.8406    |  0.8638  |\n",
            "|    breast-w    |    0.9400    |  0.9629  |\n",
            "|    breast-w    |    0.9656    |  0.9427  |\n",
            "|    breast-w    |    0.9543    |  0.9400  |\n",
            "|    breast-w    |    0.9542    |  0.9427  |\n",
            "|    breast-w    |    0.9343    |  0.9514  |\n",
            "|    breast-w    |    0.9599    |  0.9513  |\n",
            "|    breast-w    |    0.9686    |  0.9514  |\n",
            "|    breast-w    |    0.9542    |  0.9513  |\n",
            "|    breast-w    |    0.9543    |  0.9171  |\n",
            "|    breast-w    |    0.9570    |  0.9513  |\n",
            "| breast-cancer  |    0.6993    |  0.6783  |\n",
            "| breast-cancer  |    0.6713    |  0.6923  |\n",
            "| breast-cancer  |    0.6783    |  0.6993  |\n",
            "| breast-cancer  |    0.6923    |  0.6993  |\n",
            "| breast-cancer  |    0.6434    |  0.6434  |\n",
            "| breast-cancer  |    0.5734    |  0.7203  |\n",
            "| breast-cancer  |    0.6434    |  0.6434  |\n",
            "| breast-cancer  |    0.6923    |  0.6993  |\n",
            "| breast-cancer  |    0.6573    |  0.6993  |\n",
            "| breast-cancer  |    0.6993    |  0.7063  |\n",
            "|   audiology    |    0.6235    |  0.6235  |\n",
            "|   audiology    |    0.7976    |  0.8095  |\n",
            "|   audiology    |    0.8118    |  0.8235  |\n",
            "|   audiology    |    0.6667    |  0.8571  |\n",
            "|   audiology    |    0.8000    |  0.8000  |\n",
            "|   audiology    |    0.8690    |  0.8690  |\n",
            "|   audiology    |    0.8588    |  0.8353  |\n",
            "|   audiology    |    0.8214    |  0.8333  |\n",
            "|   audiology    |    0.5765    |  0.8235  |\n",
            "|   audiology    |    0.7976    |  0.7857  |\n",
            "|     lymph      |    0.7606    |  0.7324  |\n",
            "|     lymph      |    0.8169    |  0.8592  |\n",
            "|     lymph      |    0.7324    |  0.7746  |\n",
            "|     lymph      |    0.8592    |  0.8169  |\n",
            "|     lymph      |    0.7746    |  0.7746  |\n",
            "|     lymph      |    0.7183    |  0.8310  |\n",
            "|     lymph      |    0.8028    |  0.8592  |\n",
            "|     lymph      |    0.7746    |  0.7746  |\n",
            "|     lymph      |    0.8028    |  0.8028  |\n",
            "|     lymph      |    0.8028    |  0.8028  |\n",
            "|   hepatitis    |    0.8205    |  0.8462  |\n",
            "|   hepatitis    |    0.7792    |  0.8701  |\n",
            "|   hepatitis    |    0.7949    |  0.8077  |\n",
            "|   hepatitis    |    0.8182    |  0.7922  |\n",
            "|   hepatitis    |    0.8462    |  0.8077  |\n",
            "|   hepatitis    |    0.8052    |  0.8961  |\n",
            "|   hepatitis    |    0.8077    |  0.8333  |\n",
            "|   hepatitis    |    0.8052    |  0.7922  |\n",
            "|   hepatitis    |    0.8590    |  0.7949  |\n",
            "|   hepatitis    |    0.8052    |  0.7792  |\n",
            "|     labor      |    0.8621    |  0.9310  |\n",
            "|     labor      |    0.7500    |  0.7857  |\n",
            "|     labor      |    0.8276    |  0.8276  |\n",
            "|     labor      |    0.6786    |  0.8571  |\n",
            "|     labor      |    0.7586    |  0.9310  |\n",
            "|     labor      |    0.8214    |  0.8929  |\n",
            "|     labor      |    0.8621    |  0.9310  |\n",
            "|     labor      |    0.9286    |  0.8929  |\n",
            "|     labor      |    0.7586    |  0.7931  |\n",
            "|     labor      |    0.8929    |  0.8571  |\n",
            "|      iris      |    0.9733    |  0.9200  |\n",
            "|      iris      |    0.9733    |  0.9600  |\n",
            "|      iris      |    0.9467    |  0.9333  |\n",
            "|      iris      |    0.9467    |  0.9467  |\n",
            "|      iris      |    0.9733    |  0.9733  |\n",
            "|      iris      |    0.9467    |  0.9333  |\n",
            "|      iris      |    0.9333    |  0.9600  |\n",
            "|      iris      |    0.9733    |  0.9733  |\n",
            "|      iris      |    0.9467    |  0.9467  |\n",
            "|      iris      |    0.9333    |  0.9467  |\n",
            "|     letter     |    0.1756    |  0.2550  |\n",
            "|     letter     |    0.2722    |  0.2003  |\n",
            "|     letter     |    0.2588    |  0.2604  |\n",
            "|     letter     |    0.2531    |  0.2392  |\n",
            "|     letter     |    0.1947    |  0.2239  |\n",
            "|     letter     |    0.2571    |  0.2287  |\n",
            "|     letter     |    0.2883    |  0.2552  |\n",
            "|     letter     |    0.2411    |  0.2233  |\n",
            "|     letter     |    0.2864    |  0.1996  |\n",
            "|     letter     |    0.3118    |  0.2184  |\n",
            "|     vowel      |    0.1495    |  0.1576  |\n",
            "|     vowel      |    0.2606    |  0.2182  |\n",
            "|     vowel      |    0.2020    |  0.1293  |\n",
            "|     vowel      |    0.1717    |  0.1717  |\n",
            "|     vowel      |    0.1899    |  0.1293  |\n",
            "|     vowel      |    0.1960    |  0.1616  |\n",
            "|     vowel      |    0.2444    |  0.1394  |\n",
            "|     vowel      |    0.1495    |  0.1414  |\n",
            "|     vowel      |    0.3091    |  0.2404  |\n",
            "|     vowel      |    0.2444    |  0.2343  |\n",
            "|    soybean     |    0.4349    |  0.3846  |\n",
            "|    soybean     |    0.3323    |  0.3709  |\n",
            "|    soybean     |    0.4112    |  0.3817  |\n",
            "|    soybean     |    0.4095    |  0.3353  |\n",
            "|    soybean     |    0.4675    |  0.2367  |\n",
            "|    soybean     |    0.3620    |  0.3353  |\n",
            "|    soybean     |    0.3905    |  0.3964  |\n",
            "|    soybean     |    0.4036    |  0.3056  |\n",
            "|    soybean     |    0.3639    |  0.3343  |\n",
            "|    soybean     |    0.3264    |  0.3234  |\n",
            "|    segment     |    0.4208    |  0.4208  |\n",
            "|    segment     |    0.5576    |  0.4242  |\n",
            "|    segment     |    0.5524    |  0.4260  |\n",
            "|    segment     |    0.4173    |  0.4173  |\n",
            "|    segment     |    0.5515    |  0.4052  |\n",
            "|    segment     |    0.4234    |  0.4225  |\n",
            "|    segment     |    0.4346    |  0.4338  |\n",
            "|    segment     |    0.4087    |  0.4078  |\n",
            "|    segment     |    0.4199    |  0.4199  |\n",
            "|    segment     |    0.5688    |  0.5602  |\n",
            "|      vote      |    0.9404    |  0.9587  |\n",
            "|      vote      |    0.9447    |  0.9585  |\n",
            "|      vote      |    0.9450    |  0.9679  |\n",
            "|      vote      |    0.9539    |  0.9447  |\n",
            "|      vote      |    0.9358    |  0.9312  |\n",
            "|      vote      |    0.9724    |  0.9631  |\n",
            "|      vote      |    0.9495    |  0.9450  |\n",
            "|      vote      |    0.9539    |  0.9539  |\n",
            "|      vote      |    0.9541    |  0.9495  |\n",
            "|      vote      |    0.9631    |  0.9631  |\n",
            "|     sonar      |    0.7115    |  0.7404  |\n",
            "|     sonar      |    0.8173    |  0.7212  |\n",
            "|     sonar      |    0.7404    |  0.7308  |\n",
            "|     sonar      |    0.8750    |  0.7981  |\n",
            "|     sonar      |    0.8365    |  0.8077  |\n",
            "|     sonar      |    0.7500    |  0.7019  |\n",
            "|     sonar      |    0.6923    |  0.6058  |\n",
            "|     sonar      |    0.7212    |  0.7404  |\n",
            "|     sonar      |    0.7692    |  0.7981  |\n",
            "|     sonar      |    0.7788    |  0.6827  |\n",
            "|      zoo       |    1.0000    |  1.0000  |\n",
            "|      zoo       |    1.0000    |  0.9524  |\n",
            "|      zoo       |    1.0000    |  0.9762  |\n",
            "|      zoo       |    1.0000    |  1.0000  |\n",
            "|      zoo       |    1.0000    |  1.0000  |\n",
            "|      zoo       |    1.0000    |  1.0000  |\n",
            "|      zoo       |    1.0000    |  1.0000  |\n",
            "|      zoo       |    0.9524    |  0.9524  |\n",
            "|      zoo       |    1.0000    |  1.0000  |\n",
            "|      zoo       |    0.9524    |  0.9524  |\n",
            "|    vehicle     |    0.6596    |  0.5485  |\n",
            "|    vehicle     |    0.6312    |  0.4728  |\n",
            "|    vehicle     |    0.6501    |  0.5437  |\n",
            "|    vehicle     |    0.6383    |  0.5768  |\n",
            "|    vehicle     |    0.6501    |  0.5603  |\n",
            "|    vehicle     |    0.6359    |  0.5887  |\n",
            "|    vehicle     |    0.6123    |  0.4799  |\n",
            "|    vehicle     |    0.6099    |  0.5485  |\n",
            "|    vehicle     |    0.6619    |  0.5201  |\n",
            "|    vehicle     |    0.5437    |  0.5012  |\n",
            "|    waveform    |    0.7692    |  0.7724  |\n",
            "|    waveform    |    0.8040    |  0.7860  |\n",
            "|    waveform    |    0.8064    |  0.7776  |\n",
            "|    waveform    |    0.8060    |  0.7852  |\n",
            "|    waveform    |    0.8088    |  0.7808  |\n",
            "|    waveform    |    0.7872    |  0.7736  |\n",
            "|    waveform    |    0.7836    |  0.7836  |\n",
            "|    waveform    |    0.8088    |  0.7784  |\n",
            "|    waveform    |    0.7984    |  0.7816  |\n",
            "|    waveform    |    0.7936    |  0.7820  |\n",
            "| primary-tumor  |    0.3444    |  0.3311  |\n",
            "| primary-tumor  |    0.3642    |  0.3510  |\n",
            "| primary-tumor  |    0.2715    |  0.3046  |\n",
            "| primary-tumor  |    0.3576    |  0.3510  |\n",
            "| primary-tumor  |    0.2914    |  0.2781  |\n",
            "| primary-tumor  |    0.3113    |  0.3113  |\n",
            "| primary-tumor  |    0.2914    |  0.3245  |\n",
            "| primary-tumor  |    0.2649    |  0.2980  |\n",
            "| primary-tumor  |    0.3046    |  0.3576  |\n",
            "| primary-tumor  |    0.2252    |  0.3444  |\n",
            "|     splice     |    0.9254    |  0.9241  |\n",
            "|     splice     |    0.9016    |  0.9361  |\n",
            "|     splice     |    0.9348    |  0.9210  |\n",
            "|     splice     |    0.8984    |  0.9292  |\n",
            "|     splice     |    0.9053    |  0.9436  |\n",
            "|     splice     |    0.9135    |  0.9348  |\n",
            "|     splice     |    0.9241    |  0.9204  |\n",
            "|     splice     |    0.9248    |  0.9260  |\n",
            "|     splice     |    0.9386    |  0.9304  |\n",
            "|     splice     |    0.9034    |  0.9298  |\n",
            "|    ringnorm    |    0.9673    |  0.8697  |\n",
            "|    ringnorm    |    0.9654    |  0.8514  |\n",
            "|    ringnorm    |    0.9703    |  0.8586  |\n",
            "|    ringnorm    |    0.9689    |  0.8624  |\n",
            "|    ringnorm    |    0.9708    |  0.8497  |\n",
            "|    ringnorm    |    0.9657    |  0.8668  |\n",
            "|    ringnorm    |    0.9724    |  0.8597  |\n",
            "|    ringnorm    |    0.9757    |  0.8649  |\n",
            "|    ringnorm    |    0.9703    |  0.8570  |\n",
            "|    ringnorm    |    0.9678    |  0.8570  |\n",
            "|     col10      |    0.6396    |  0.6436  |\n",
            "|     col10      |    0.6085    |  0.6343  |\n",
            "|     col10      |    0.6980    |  0.7069  |\n",
            "|     col10      |    0.6561    |  0.6561  |\n",
            "|     col10      |    0.6673    |  0.6703  |\n",
            "|     col10      |    0.6829    |  0.6878  |\n",
            "|     col10      |    0.6337    |  0.6495  |\n",
            "|     col10      |    0.6779    |  0.6819  |\n",
            "|     col10      |    0.6911    |  0.6911  |\n",
            "|     col10      |    0.6492    |  0.6819  |\n",
            "|      cmc       |    0.5400    |  0.5278  |\n",
            "|      cmc       |    0.5435    |  0.5448  |\n",
            "|      cmc       |    0.5061    |  0.5251  |\n",
            "|      cmc       |    0.5476    |  0.5421  |\n",
            "|      cmc       |    0.5482    |  0.5550  |\n",
            "|      cmc       |    0.5027    |  0.5258  |\n",
            "|      cmc       |    0.5387    |  0.5441  |\n",
            "|      cmc       |    0.5285    |  0.5408  |\n",
            "|      cmc       |    0.5251    |  0.5115  |\n",
            "|      cmc       |    0.5149    |  0.5122  |\n",
            "|    column3C    |    0.6129    |  0.6000  |\n",
            "|    column3C    |    0.6516    |  0.6516  |\n",
            "|    column3C    |    0.8194    |  0.7871  |\n",
            "|    column3C    |    0.7613    |  0.4516  |\n",
            "|    column3C    |    0.7871    |  0.6516  |\n",
            "|    column3C    |    0.7097    |  0.4258  |\n",
            "|    column3C    |    0.5548    |  0.5548  |\n",
            "|    column3C    |    0.6774    |  0.6516  |\n",
            "|    column3C    |    0.7935    |  0.5161  |\n",
            "|    column3C    |    0.7871    |  0.7484  |\n",
            "|     ecoli      |    0.8049    |  0.7500  |\n",
            "|     ecoli      |    0.5951    |  0.6994  |\n",
            "|     ecoli      |    0.7927    |  0.6829  |\n",
            "|     ecoli      |    0.6196    |  0.6871  |\n",
            "|     ecoli      |    0.7561    |  0.4207  |\n",
            "|     ecoli      |    0.6564    |  0.6564  |\n",
            "|     ecoli      |    0.6463    |  0.6524  |\n",
            "|     ecoli      |    0.4479    |  0.7362  |\n",
            "|     ecoli      |    0.7012    |  0.7500  |\n",
            "|     ecoli      |    0.6933    |  0.6933  |\n",
            "|      monk      |    0.7869    |  0.8197  |\n",
            "|      monk      |    0.8689    |  0.9180  |\n",
            "|      monk      |    0.8525    |  0.8852  |\n",
            "|      monk      |    0.8689    |  0.8852  |\n",
            "|      monk      |    0.9344    |  0.8689  |\n",
            "|      monk      |    0.8852    |  0.9180  |\n",
            "|      monk      |    0.9344    |  0.9344  |\n",
            "|      monk      |    0.9344    |  0.7377  |\n",
            "|      monk      |    0.8689    |  0.9344  |\n",
            "|      monk      |    0.8689    |  0.8689  |\n",
            "|  transfusion   |    0.7914    |  0.7620  |\n",
            "|  transfusion   |    0.7701    |  0.7861  |\n",
            "|  transfusion   |    0.7861    |  0.7941  |\n",
            "|  transfusion   |    0.7594    |  0.7834  |\n",
            "|  transfusion   |    0.8102    |  0.7834  |\n",
            "|  transfusion   |    0.7620    |  0.7273  |\n",
            "|  transfusion   |    0.7754    |  0.7701  |\n",
            "|  transfusion   |    0.7701    |  0.7513  |\n",
            "|  transfusion   |    0.7701    |  0.7567  |\n",
            "|  transfusion   |    0.7807    |  0.7807  |\n",
            "| wineCultivars  |    0.9610    |  0.9221  |\n",
            "| wineCultivars  |    0.9079    |  0.8947  |\n",
            "| wineCultivars  |    0.9740    |  0.8961  |\n",
            "| wineCultivars  |    0.9737    |  0.9079  |\n",
            "| wineCultivars  |    0.9740    |  0.8052  |\n",
            "| wineCultivars  |    0.9079    |  0.8816  |\n",
            "| wineCultivars  |    0.9481    |  0.8571  |\n",
            "| wineCultivars  |    0.9474    |  0.8947  |\n",
            "| wineCultivars  |    0.9221    |  0.8701  |\n",
            "| wineCultivars  |    0.9737    |  0.6579  |\n",
            "|   hillValley   |    0.9802    |  0.5017  |\n",
            "|   hillValley   |    0.9670    |  0.4983  |\n",
            "|   hillValley   |    0.9736    |  0.4950  |\n",
            "|   hillValley   |    0.9835    |  0.4620  |\n",
            "|   hillValley   |    0.9901    |  0.5149  |\n",
            "|   hillValley   |    0.9670    |  0.4950  |\n",
            "|   hillValley   |    0.9868    |  0.5248  |\n",
            "|   hillValley   |    0.9670    |  0.5083  |\n",
            "|   hillValley   |    0.9439    |  0.4950  |\n",
            "|   hillValley   |    0.9670    |  0.5215  |\n",
            "| movementLibras |    0.1444    |  0.1111  |\n",
            "| movementLibras |    0.3111    |  0.1667  |\n",
            "| movementLibras |    0.2056    |  0.1444  |\n",
            "| movementLibras |    0.1389    |  0.0611  |\n",
            "| movementLibras |    0.1889    |  0.1667  |\n",
            "| movementLibras |    0.1889    |  0.0833  |\n",
            "| movementLibras |    0.1500    |  0.0889  |\n",
            "| movementLibras |    0.0833    |  0.1111  |\n",
            "| movementLibras |    0.2000    |  0.1222  |\n",
            "| movementLibras |    0.2056    |  0.0722  |\n",
            "|    spambase    |    0.9048    |  0.8870  |\n",
            "|    spambase    |    0.8883    |  0.8943  |\n",
            "|    spambase    |    0.8887    |  0.8840  |\n",
            "|    spambase    |    0.8943    |  0.8935  |\n",
            "|    spambase    |    0.9027    |  0.8987  |\n",
            "|    spambase    |    0.8930    |  0.8957  |\n",
            "|    spambase    |    0.8935    |  0.9018  |\n",
            "|    spambase    |    0.8935    |  0.9000  |\n",
            "|    spambase    |    0.8966    |  0.8979  |\n",
            "|    spambase    |    0.8961    |  0.8965  |\n",
            "|     yeast      |    0.4014    |  0.4500  |\n",
            "|     yeast      |    0.4235    |  0.4235  |\n",
            "|     yeast      |    0.4230    |  0.4230  |\n",
            "|     yeast      |    0.4344    |  0.4398  |\n",
            "|     yeast      |    0.5162    |  0.4000  |\n",
            "|     yeast      |    0.4032    |  0.4032  |\n",
            "|     yeast      |    0.4446    |  0.4338  |\n",
            "|     yeast      |    0.4114    |  0.4398  |\n",
            "|     yeast      |    0.4135    |  0.4824  |\n",
            "|     yeast      |    0.4601    |  0.4587  |\n",
            "|  iphonetweets  |    0.6955    |  0.6955  |\n",
            "|  iphonetweets  |    0.7368    |  0.7331  |\n",
            "|  iphonetweets  |    0.7030    |  0.7030  |\n",
            "|  iphonetweets  |    0.6805    |  0.6805  |\n",
            "|  iphonetweets  |    0.7293    |  0.7218  |\n",
            "|  iphonetweets  |    0.7030    |  0.7030  |\n",
            "|  iphonetweets  |    0.7180    |  0.7293  |\n",
            "|  iphonetweets  |    0.7105    |  0.7105  |\n",
            "|  iphonetweets  |    0.6917    |  0.6992  |\n",
            "|  iphonetweets  |    0.7293    |  0.7293  |\n",
            "|  hobbittweets  |    0.9464    |  0.9464  |\n",
            "|  hobbittweets  |    0.9464    |  0.9464  |\n",
            "|  hobbittweets  |    0.9425    |  0.9425  |\n",
            "|  hobbittweets  |    0.9464    |  0.9464  |\n",
            "|  hobbittweets  |    0.9387    |  0.9387  |\n",
            "|  hobbittweets  |    0.9579    |  0.9579  |\n",
            "|  hobbittweets  |    0.9349    |  0.9349  |\n",
            "|  hobbittweets  |    0.9464    |  0.9464  |\n",
            "|  hobbittweets  |    0.9502    |  0.9540  |\n",
            "|  hobbittweets  |    0.9387    |  0.9310  |\n",
            "+----------------+--------------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9dXJSK_vTjY"
      },
      "source": [
        "np.save(ds_folderpath + \"/\" + 'adaboost.npy',foldPreds_AB ) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjkOqaARshcG"
      },
      "source": [
        "Sonuçlar\r\n",
        "-----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVJWXRV9O43j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234b4b37-6328-4d63-967a-f926dd65668b"
      },
      "source": [
        "foldPreds_Bagging = np.load(\"gdrive/MyDrive/ImprovedSpace/\" + 'bagging.npy',allow_pickle='TRUE').item()\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "result_Bagging = PrettyTable()\r\n",
        "\r\n",
        "result_Bagging.field_names = [\"Dataset\",\"Win_Improved\",\"Loss_Improved\" ,\"Imp_Bagging\", \"Bagging\"]\r\n",
        "for ds in datasets:\r\n",
        "  win = np.array(foldPreds_Bagging[ds]) < np.array(foldPreds_Bagging[ds+'_imp'])\r\n",
        "  loss = np.array(foldPreds_Bagging[ds]) > np.array(foldPreds_Bagging[ds+'_imp'])\r\n",
        "  cnt_win= np.count_nonzero(win)\r\n",
        "  cnt_loss= np.count_nonzero(loss)\r\n",
        "  ave= np.average(foldPreds_Bagging[ds])\r\n",
        "  ave_imp= np.average(foldPreds_Bagging[ds+\"_imp\"])\r\n",
        "  result_Bagging.add_row((ds, cnt_win,cnt_loss, \"%.4f\" % ave_imp, \"%.4f\" % ave))\r\n",
        "\r\n",
        "print(result_Bagging)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------+---------------+-------------+---------+\n",
            "|    Dataset     | Win_Improved | Loss_Improved | Imp_Bagging | Bagging |\n",
            "+----------------+--------------+---------------+-------------+---------+\n",
            "|   ionosphere   |      8       |       1       |    0.9293   |  0.9094 |\n",
            "|    diabetes    |      7       |       3       |    0.7456   |  0.7385 |\n",
            "|     glass      |      3       |       7       |    0.6665   |  0.7091 |\n",
            "|    abalone     |      8       |       2       |    0.2350   |  0.2293 |\n",
            "| heart-statlog  |      8       |       2       |    0.8074   |  0.7837 |\n",
            "|     colic      |      5       |       5       |    0.8359   |  0.8315 |\n",
            "|    credit-g    |      4       |       6       |    0.7308   |  0.7340 |\n",
            "| balance-scale  |      10      |       0       |    0.9274   |  0.8089 |\n",
            "|     autos      |      3       |       5       |    0.6634   |  0.6644 |\n",
            "|    credit-a    |      4       |       5       |    0.8470   |  0.8490 |\n",
            "|    breast-w    |      9       |       1       |    0.9637   |  0.9551 |\n",
            "| breast-cancer  |      7       |       3       |    0.6979   |  0.6916 |\n",
            "|   audiology    |      7       |       2       |    0.8758   |  0.8533 |\n",
            "|     lymph      |      5       |       4       |    0.8183   |  0.8070 |\n",
            "|   hepatitis    |      7       |       2       |    0.8309   |  0.8038 |\n",
            "|     labor      |      4       |       3       |    0.8738   |  0.8667 |\n",
            "|      iris      |      5       |       1       |    0.9640   |  0.9467 |\n",
            "|     letter     |      5       |       5       |    0.9089   |  0.9087 |\n",
            "|     vowel      |      10      |       0       |    0.8396   |  0.8109 |\n",
            "|    soybean     |      7       |       3       |    0.9129   |  0.9123 |\n",
            "|    segment     |      10      |       0       |    0.9706   |  0.9591 |\n",
            "|      vote      |      5       |       3       |    0.9540   |  0.9540 |\n",
            "|     sonar      |      8       |       1       |    0.7952   |  0.7385 |\n",
            "|      zoo       |      3       |       0       |    0.9976   |  0.9857 |\n",
            "|    vehicle     |      10      |       0       |    0.7430   |  0.7076 |\n",
            "|    waveform    |      10      |       0       |    0.8297   |  0.8056 |\n",
            "| primary-tumor  |      5       |       4       |    0.4497   |  0.4285 |\n",
            "|     splice     |      7       |       2       |    0.9461   |  0.9436 |\n",
            "|    ringnorm    |      10      |       0       |    0.9775   |  0.9338 |\n",
            "|     col10      |      6       |       4       |    0.7763   |  0.7754 |\n",
            "|      cmc       |      8       |       2       |    0.5256   |  0.5120 |\n",
            "|    column3C    |      6       |       3       |    0.8277   |  0.8181 |\n",
            "|     ecoli      |      8       |       2       |    0.8416   |  0.8269 |\n",
            "|      monk      |      5       |       5       |    0.8951   |  0.9049 |\n",
            "|  transfusion   |      7       |       3       |    0.7398   |  0.7278 |\n",
            "| wineCultivars  |      6       |       2       |    0.9621   |  0.9333 |\n",
            "|   hillValley   |      10      |       0       |    0.9749   |  0.5198 |\n",
            "| movementLibras |      10      |       0       |    0.7350   |  0.6433 |\n",
            "|    spambase    |      9       |       0       |    0.9257   |  0.9162 |\n",
            "|     yeast      |      6       |       3       |    0.5652   |  0.5607 |\n",
            "|  iphonetweets  |      6       |       4       |    0.7508   |  0.7459 |\n",
            "|  hobbittweets  |      3       |       1       |    0.9330   |  0.9310 |\n",
            "+----------------+--------------+---------------+-------------+---------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ58Bd0KqNAb",
        "outputId": "335a5b39-c52f-4e3c-a3b7-028b99cab0c2"
      },
      "source": [
        "foldPreds_RF = np.load(\"gdrive/MyDrive/ImprovedSpace/\" + 'RF.npy',allow_pickle='TRUE').item()\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "result_RF = PrettyTable()\r\n",
        "\r\n",
        "result_RF.field_names = [\"Dataset\",\"Win_Improved\",\"Loss_Improved\" ,\"Imp_RF\", \"RF\"]\r\n",
        "for ds in datasets:\r\n",
        "  win = np.array(foldPreds_RF[ds]) < np.array(foldPreds_RF[ds+'_imp'])\r\n",
        "  loss = np.array(foldPreds_RF[ds]) > np.array(foldPreds_RF[ds+'_imp'])\r\n",
        "  cnt_win= np.count_nonzero(win)\r\n",
        "  cnt_loss= np.count_nonzero(loss)\r\n",
        "  ave= np.average(foldPreds_RF[ds])\r\n",
        "  ave_imp= np.average(foldPreds_RF[ds+\"_imp\"])\r\n",
        "  result_RF.add_row((ds, cnt_win, cnt_loss, \"%.4f\" % ave_imp, \"%.4f\" % ave))\r\n",
        "\r\n",
        "print(result_RF)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------+---------------+--------+--------+\n",
            "|    Dataset     | Win_Improved | Loss_Improved | Imp_RF |   RF   |\n",
            "+----------------+--------------+---------------+--------+--------+\n",
            "|   ionosphere   |      7       |       2       | 0.9310 | 0.9168 |\n",
            "|    diabetes    |      9       |       1       | 0.7440 | 0.7320 |\n",
            "|     glass      |      6       |       3       | 0.7045 | 0.7014 |\n",
            "|    abalone     |      5       |       5       | 0.2276 | 0.2270 |\n",
            "| heart-statlog  |      7       |       3       | 0.8037 | 0.7874 |\n",
            "|     colic      |      1       |       6       | 0.8082 | 0.8158 |\n",
            "|    credit-g    |      7       |       3       | 0.7378 | 0.7282 |\n",
            "| balance-scale  |      10      |       0       | 0.9261 | 0.8038 |\n",
            "|     autos      |      4       |       6       | 0.6614 | 0.6683 |\n",
            "|    credit-a    |      4       |       5       | 0.8583 | 0.8606 |\n",
            "|    breast-w    |      9       |       1       | 0.9642 | 0.9548 |\n",
            "| breast-cancer  |      4       |       6       | 0.7007 | 0.7098 |\n",
            "|   audiology    |      7       |       1       | 0.8580 | 0.8201 |\n",
            "|     lymph      |      4       |       6       | 0.8394 | 0.8310 |\n",
            "|   hepatitis    |      6       |       2       | 0.8309 | 0.8194 |\n",
            "|     labor      |      3       |       3       | 0.8950 | 0.9021 |\n",
            "|      iris      |      4       |       4       | 0.9600 | 0.9560 |\n",
            "|     letter     |      1       |       9       | 0.9089 | 0.9159 |\n",
            "|     vowel      |      10      |       0       | 0.8564 | 0.8251 |\n",
            "|    soybean     |      5       |       5       | 0.9099 | 0.9120 |\n",
            "|    segment     |      8       |       2       | 0.9695 | 0.9669 |\n",
            "|      vote      |      3       |       5       | 0.9545 | 0.9568 |\n",
            "|     sonar      |      8       |       2       | 0.7865 | 0.7519 |\n",
            "|      zoo       |      0       |       3       | 0.9929 | 1.0000 |\n",
            "|    vehicle     |      9       |       1       | 0.7487 | 0.7125 |\n",
            "|    waveform    |      10      |       0       | 0.8293 | 0.8097 |\n",
            "| primary-tumor  |      8       |       2       | 0.4497 | 0.4278 |\n",
            "|     splice     |      9       |       1       | 0.9328 | 0.9206 |\n",
            "|    ringnorm    |      10      |       0       | 0.9779 | 0.9339 |\n",
            "|     col10      |      5       |       5       | 0.7805 | 0.7796 |\n",
            "|      cmc       |      6       |       4       | 0.5195 | 0.5130 |\n",
            "|    column3C    |      7       |       3       | 0.8439 | 0.8194 |\n",
            "|     ecoli      |      9       |       1       | 0.8594 | 0.8349 |\n",
            "|      monk      |      6       |       3       | 0.9082 | 0.9016 |\n",
            "|  transfusion   |      5       |       3       | 0.7401 | 0.7332 |\n",
            "| wineCultivars  |      8       |       2       | 0.9634 | 0.9477 |\n",
            "|   hillValley   |      10      |       0       | 0.9739 | 0.5228 |\n",
            "| movementLibras |      7       |       2       | 0.6889 | 0.6400 |\n",
            "|    spambase    |      3       |       5       | 0.9227 | 0.9228 |\n",
            "|     yeast      |      6       |       4       | 0.5711 | 0.5623 |\n",
            "|  iphonetweets  |      5       |       4       | 0.7590 | 0.7575 |\n",
            "|  hobbittweets  |      3       |       7       | 0.9199 | 0.9272 |\n",
            "+----------------+--------------+---------------+--------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KunTufROqQa0",
        "outputId": "3c4af991-af58-4deb-f7d6-7544fe4b9b28"
      },
      "source": [
        "foldPreds_AB = np.load(\"gdrive/MyDrive/ImprovedSpace/\" + 'adaboost.npy',allow_pickle='TRUE').item()\r\n",
        "from prettytable import PrettyTable\r\n",
        "    \r\n",
        "result_AB = PrettyTable()\r\n",
        "\r\n",
        "result_AB.field_names = [\"Dataset\",\"Win_Improved\",\"Loss_Improved\" ,\"Imp_Adaboost\", \"Adaboost\"]\r\n",
        "for ds in datasets:\r\n",
        "  win = np.array(foldPreds_AB[ds]) < np.array(foldPreds_AB[ds+'_imp'])\r\n",
        "  loss = np.array(foldPreds_AB[ds]) > np.array(foldPreds_AB[ds+'_imp'])\r\n",
        "  cnt_win= np.count_nonzero(win)\r\n",
        "  cnt_loss= np.count_nonzero(loss)\r\n",
        "  ave= np.average(foldPreds_AB[ds])\r\n",
        "  ave_imp= np.average(foldPreds_AB[ds+\"_imp\"])\r\n",
        "  result_AB.add_row((ds, cnt_win,cnt_loss, \"%.4f\" % ave_imp, \"%.4f\" % ave))\r\n",
        "\r\n",
        "print(result_AB)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------+---------------+--------------+----------+\n",
            "|    Dataset     | Win_Improved | Loss_Improved | Imp_Adaboost | Adaboost |\n",
            "+----------------+--------------+---------------+--------------+----------+\n",
            "|   ionosphere   |      6       |       1       |    0.9168    |  0.9048  |\n",
            "|    diabetes    |      7       |       3       |    0.7495    |  0.7375  |\n",
            "|     glass      |      9       |       1       |    0.5005    |  0.4294  |\n",
            "|    abalone     |      6       |       4       |    0.2199    |  0.2225  |\n",
            "| heart-statlog  |      4       |       5       |    0.7859    |  0.7926  |\n",
            "|     colic      |      2       |       7       |    0.7804    |  0.7880  |\n",
            "|    credit-g    |      3       |       7       |    0.7070    |  0.7140  |\n",
            "| balance-scale  |      6       |       4       |    0.8220    |  0.8518  |\n",
            "|     autos      |      7       |       3       |    0.4505    |  0.4267  |\n",
            "|    credit-a    |      5       |       5       |    0.8414    |  0.8397  |\n",
            "|    breast-w    |      8       |       2       |    0.9542    |  0.9462  |\n",
            "| breast-cancer  |      1       |       7       |    0.6650    |  0.6881  |\n",
            "|   audiology    |      2       |       5       |    0.7623    |  0.8061  |\n",
            "|     lymph      |      2       |       4       |    0.7845    |  0.8028  |\n",
            "|   hepatitis    |      5       |       5       |    0.8141    |  0.8220  |\n",
            "|     labor      |      2       |       7       |    0.8140    |  0.8700  |\n",
            "|      iris      |      4       |       2       |    0.9547    |  0.9493  |\n",
            "|     letter     |      7       |       3       |    0.2539    |  0.2304  |\n",
            "|     vowel      |      8       |       1       |    0.2117    |  0.1723  |\n",
            "|    soybean     |      8       |       2       |    0.3902    |  0.3404  |\n",
            "|    segment     |      7       |       0       |    0.4755    |  0.4338  |\n",
            "|      vote      |      5       |       3       |    0.9513    |  0.9536  |\n",
            "|     sonar      |      7       |       3       |    0.7692    |  0.7327  |\n",
            "|      zoo       |      2       |       0       |    0.9905    |  0.9833  |\n",
            "|    vehicle     |      10      |       0       |    0.6293    |  0.5340  |\n",
            "|    waveform    |      8       |       1       |    0.7966    |  0.7801  |\n",
            "| primary-tumor  |      4       |       5       |    0.3026    |  0.3252  |\n",
            "|     splice     |      4       |       6       |    0.9170    |  0.9295  |\n",
            "|    ringnorm    |      10      |       0       |    0.9695    |  0.8597  |\n",
            "|     col10      |      0       |       8       |    0.6604    |  0.6703  |\n",
            "|      cmc       |      4       |       6       |    0.5295    |  0.5329  |\n",
            "|    column3C    |      8       |       0       |    0.7155    |  0.6039  |\n",
            "|     ecoli      |      3       |       5       |    0.6713    |  0.6728  |\n",
            "|      monk      |      2       |       6       |    0.8803    |  0.8770  |\n",
            "|  transfusion   |      6       |       3       |    0.7775    |  0.7695  |\n",
            "| wineCultivars  |      10      |       0       |    0.9490    |  0.8587  |\n",
            "|   hillValley   |      10      |       0       |    0.9726    |  0.5017  |\n",
            "| movementLibras |      9       |       1       |    0.1817    |  0.1128  |\n",
            "|    spambase    |      4       |       6       |    0.8952    |  0.8949  |\n",
            "|     yeast      |      3       |       4       |    0.4331    |  0.4354  |\n",
            "|  iphonetweets  |      2       |       2       |    0.7098    |  0.7105  |\n",
            "|  hobbittweets  |      1       |       1       |    0.9448    |  0.9444  |\n",
            "+----------------+--------------+---------------+--------------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}